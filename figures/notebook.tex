
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{ATG\_data\_science\_take\_home\_ML}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{ATG Data Scientist
Challenge}\label{atg-data-scientist-challenge}

Thank you for your interest in joining the data science team at Uber
ATG. The next step is to complete Uber ATG's take home exercise. This
will allow you to get an idea of what it's like to work for us while
showcasing your statistics, programming, and data analysis capabilities.

While we invite you to fill out the remainder of this notebook for your
submission, you may send your results back in any format as long as the
work/code/analysis is \textbf{reproducible}. We've had candidates submit
RMarkdown HTMLs or LaTeX generated PDFs as well. You may use any
language (or Jupyter Kernel) you want but keep in mind that we primarily
do deployment, engineering, and analysis work in Python.

There is no time limit, but please try to send back the completed
assignment within 1 week of receiving it. Please delete any data you
have downloaded from us after submitting the assignment.

If you have any questions about the assignment, please reach out to your
recruiter.

Thanks, ATG Data Science

    \section{Driver Signup Analysis}\label{driver-signup-analysis}

You can use the csv: * \texttt{ds\_challenge\_v2\_data.csv}

included in the zip file included with this notebook or download the
data set at the following link:

\begin{itemize}
\tightlist
\item
  \href{https://drive.google.com/a/uber.com/file/d/0BxkZqrCogcyWbUs2Smhlc0VSams/view?usp=drive_web}{\textbf{Dataset
  Download Link}}
\end{itemize}

Uber's Driver team is interested in predicting which driver signups are
most likely to start driving. To help explore this question, we have
provided a sample dataset of a cohort of driver signups in January 2015.
The data was pulled a few months after they signed up to include the
result of whether they actually completed their first trip. It also
includes several pieces of background information gathered about the
driver and their car.

See below for a detailed description of the dataset:

\begin{itemize}
\tightlist
\item
  \textbf{id:} driver\_id
\item
  \textbf{city\_id:} city\_id this user signed up in
\item
  \textbf{signup\_os:} signup device of the user (``android'', ``ios'',
  ``website'', ``other'')
\item
  \textbf{signup\_channel:} what channel did the driver sign up from
  (``offline'', ``paid'', ``organic'', ``referral'')
\item
  \textbf{signup\_timestamp:} timestamp of account creation; local time
  in the form `YYYY-MM-DD'
\item
  \textbf{bgc\_date:} date of background check consent; in the form
  `YYYY-MM-DD'
\item
  \textbf{vehicle\_added\_date:} date when driver's vehicle information
  was uploaded (by the driver); in the form `YYYY-MM-DD'
\item
  \textbf{first\_trip\_date:} date of the first trip as a driver; in the
  form `YYYY-MM-DD'
\item
  \textbf{vehicle\_model:} model of vehicle uploaded (i.e. Accord,
  Prius, 350z)
\item
  \textbf{vehicle\_year:} year that the car was made; in the form `YYYY'
\end{itemize}

\textbf{Our primary goal is to understand what factors are best at
predicting whether a signup will start to drive, and offer suggestions
to operationalize those insights to help Uber}. This take home consists
of answering three main tasks with some discussion questions under them
to get you started.

Ordering and presentation format is up to you, but we love analyses that
are well organized and have a linear flow from data ingestion to final
result(s). We especially look for well stated assumptions and an eye for
business/product impact of any analysis or model.

    \section{Abstract :}\label{abstract}

This notebook is dedicated to the solution of Uber take home challenge.
This challenge consists of three main questions and several
sub-questions. Here is a list:

1 - \textbf{Conduct an exploratory analysis of the data to give us
qualitative and quantitative insights.}

2 - \textbf{Build a statistical model to predict whether a driver that
signed up will begin driving for Uber.} - How did the model perform? Are
there any caveats? How can Uber use your model to improve our product?

3 - \textbf{Build a model to forecast the number of new drivers we
expect to start every week.} - How would you validate a model like this?
What other information would you use if you had access to all of Uber's
data?

My purpose in this study is to keep everything in line with the business
question we like to answer. For example, EDA is performed to bring
insights that are related to our business questions (task2 and task3).
Therefore, I merged the EDA question to the next two questions. Here is
a summary of our findings for each task. If you like to read the details
of each step, please keep reading.

\subsection{Abstract - Task 1:}\label{abstract---task-1}

\textbf{Conduct an exploratory analysis of the data to give us
qualitative and quantitative insights.} - Does all the data make sense?
Did you have to throw anything away? Are there interesting patterns that
emerge?

\begin{itemize}
\tightlist
\item
  The data was very clean. I noticed only one outlier in the
  \texttt{vehicle\_year} column.
\item
  A new column, named \texttt{target}, is generated based on the
  \texttt{first\_completed\_date}. The value of the target is 1 when the
  date is availble and 0 otherwise. This was used as our dependant
  (target) variable we want to explain in the first task.
\item
  The target variable was very well distributed throughout the 30 days
  of observation and I concluded that a random train-holdout split is
  sufficient.
\item
  There are 54681 observations and 11 features in the data set and large
  portion of the data is missing.
\item
  11.22\% of the registrators become Uber driver. Therefore, there is an
  excellent margin of growth for the product if right insights are
  found.
\item
  Customers are from 3 cities, use 5 type of operating systems, and come
  from 3 signup chanels. Moreover, there are 46 vehicle make that signed
  up. Toyota is the most popular vehicle make and Honda-Civic is the
  most popular vehicle model.
\item
  Registration time is limited to the month of January 2016. However, we
  observed activation date is from the January 1st, 2016 through March
  26th, 2016. Two more date time features are also provided: time of
  background check consent, and the time when vehicle information was
  uploaded.
\item
  There is about 48.4 of duplicate observations. Since there are small
  number of features, it is not easy to make an assumption for the
  origin of duplicates. Specially, when for large number of rows,
  several variables are (equally) missing. However, it is interesting to
  know that for non-duplicate columns, the ratio of becoming an Uber
  driver increases from 11.22\% to 21.75\% percent. This information is
  later used for developing new features.
\item
  Although large portion of data is missing, we noticed that there is a
  rich amount of information in those missing values. For example, for
  observations with missing operative system data, the ratio of becoming
  Uber driver is 0.02 which is almost 1/6 of the whole data.
\item
  Almost 40\% of background checkced column is missing. There is a vast
  welath of information in this column, however. Poeple who didn't
  consent the bacground check have 0.007 probablity of becoming an UBER
  driver. This is 16 times smaller than the average of the population.
  \textbf{Uber might need to ease the background check process by
  providing more ways of providing information. Moreover, it might help
  to tell the customers about how their data will be dealt with.
  Finally, UBER might be able to provide different level of background
  check for different services so that more people can become a driver
  with a peace of mind.}
\item
  Almost 75\% of the vehicle\_added\_date column is missing. When the
  data is not missing, 44\% of the customers become Uber driver.
  \textbf{Uber might be able to send follow up emails to the
  registrators and ask them to upload their information. Customer might
  commit more to the program by uploading their information. Moreover,
  streamlining the process might make it easier for the users to become
  Uber driver.}
\item
  People using mac os have highest probablity of becomign Uber driver.
  The reason might be due to the possiblity of mac users being a part of
  a population that would like to stay up to date with the new
  technologies. Moreover, in general the phone users (android web, ios
  web) seem to be less prone to becoming UBER drivers. \textbf{UBER
  might need to improve the quality of their phone websites to make the
  experience more appealing for those users}.
\item
  I noticed that the newer cars have higher probablity of becoming an
  Uber driver. The median is around 2013. \textbf{This might be due to
  the vehicle age requirements for drivers that are joining Uber. Uber
  might suggest that if the age of the car is above a certain
  threshould, the driver must provide additional information, barometer,
  car fax info, a certification of vehicle quality to prove that the
  vehicle is in good shape. Recall more than only 50 percent of the cars
  that registered are more than 3 years old.}
\item
  Going forward, although graphically speaking, the city name doesn't
  affect the target variable (becoming Uber driver), we see that this
  feature is stitistically significant in seperating the classes.
\end{itemize}

    \subsection{Abstract - Task 2:}\label{abstract---task-2}

\textbf{Build a statistical model to predict whether a driver that
signed up will begin driving for Uber.} - How did the model perform? Are
there any caveats? How can Uber use your model to improve our product?

\begin{itemize}
\item
  I used the insights from EDA section and engineered several features.
  Most informative one was the time interval between registration and
  the upload of information. The next informative feature was whether
  \texttt{vehicle\_added\_date} column was missing, and the third one
  was the time diffference between the time of registration and the time
  that background was checked. \textbf{As discussed in the previous
  task, the first few days after regitration is the golden time for Uber
  to follow up with the users and also to facilitiate the background and
  data upload processes.}
\item
  The next important feature was vehicle age. Younger vehilces have
  higher probablity of becoming a driver.In Uber, we can rethink how to
  involve older vehicles.
\item
  After generating 19 features, principal component analysis was used to
  visually demonstrate whether features carry enough information.
  Although several features explained small variance of the data, in
  total large portion of features were meaningful and the PCA graph did
  not totally saturate.
\item
  Then feature importance from random forest was used to get and
  undrestanding of where the discriminative features are located.
\item
  A simple logistic regression was used to check whether the feature set
  results in overfitting. We noticed that the features don't result in
  overfitting as the train and test accuracy were close. However, the
  model was not able to capture all the variance in target variable. We
  ended up with area under ROC curve of around \(97\%\).
\item
  Then 3 models are hypertunned, namely, SVM, KNN, random forest, to
  assure that different fature characteristics are captured. The AUC
  resulst for these three was very similar to logistic regiression and
  \(AUC=0.97\%\)
\item
  Although it could be concluded from the previous models that the
  features are seperatinng the two classes well and complicating models
  doesn't improve the model, I went ahead and hypertunned a gradient
  boosted trees model to assure that this is the case. Again similar
  result was achieved.
\end{itemize}

** Conclusion**:

Model performed acceptable in predicting the very skewed classification
problem. It reached area under AUC curve equal to 97\%. There were
however, some assumptions in the process that needs to be confirmed. For
example, we assumed that all the missing values are data not
provided/available by the user. Therefore, we assumed that no missing
value occured due to any other data pipelining complications.
\textbf{Moreover, the population of study, customers who registered in
January, are not generalizable to all the users who register in Uber
service. The main reason is that January is especial month as it is in
the middle of winter and also right after holidays and usually people
have quite different behaviour during this period of time.}

Finally, although model is designed for a particular time of the year,
there seems to be some insights that Uber can use to improve its
service. \textbf{First, as mentioned before, the background check can be
revisited. Several levels of background for different services assure
that different group of users are engaged and satisfied. Similarly,
vehicle age can be revisited. For example, different services can ask
for different age. This assures that users with older vehicles are also
engaged in this service. Finally, we noticed that the first couple of
days after registration is critical in defining whether a registered
user would become a driver. Therefore, incentive, discounts, and other
follow up actions can be focused on these days.}

    \subsection{Abstract - Task 3:}\label{abstract---task-3}

3 - Build a model to forecast the number of new drivers we expect to
start every week. - How would you validate a model like this? What other
information would you use if you had access to all of Uber's data?

This was a very interesting task. The answer to this question has real
business value. It can help the business team to see whether we are
close to the financial goals and whether corrective actions are
required.

The question was also interesting from another perspective. Only 9 weeks
(9 observations) exist in the model. One caveat was that youd dont want
the outliers to dominate the model and at the same time we didn't have
the luxuray to drop any observation. Here is an abstract of the solution
for this task.

\begin{itemize}
\tightlist
\item
  We used the \texttt{first\_completed\_date} feature to generate the
  number of weekly activations.
\item
  We noticed that the cumulative number of registration in the past
  thirty days has a very similar trend as the number of activation.
\item
  Since the number of observations in this dataset is not enough,
  bootstraping samples was used to develope \(95\%\) confidence
  intervals around slope and intercepts.
\item
  One of the observations was randomly dropped, and used to demonstrate
  a of validation report.
\item
  The validation would, however, be different for larger data sets.
  Larger observations allows using actual time series forecasting. In
  thoese cases our holdout set is different. It consists of periodically
  captured weekly observations, say each month we get one week and
  iteratively train the model for all the data before that particular
  week.
\item
  Using the median slope and intercept, we expained 98\% of the variance
  in target (\(R^2=98\)) which is an acceptable result considering the
  small number of observations and a single explanatory vairbale.
\item
  Since there was not enough number of observations, we didn't go for
  further features as we would have lose another degree of freedom for
  that feature and our model was doing pretty well explaining the
  variance.
\item
  This model, however, has limitations. First, it is not generalizable
  to other months of the year. As we explained before, month of January
  is speciall as it is in the middle of winter and also it is right
  after the holidays. Moreover, we didnt capture enough priodicity in
  the data. If we have more observations, say for 3 years, we can easily
  use already available techniques e.g. ARIMA to predict the time
  series.
\end{itemize}

    \textbf{\emph{\[If\ interested\ in\ the\ details\ of\ each\ task,\ please\ read\ the\ following\ sections.\]}}

    \section{Details - Task 2:}\label{details---task-2}

Build a statistical model to predict whether a driver that signed up
will begin driving for Uber. How did the model perform? Are there any
caveats? How can Uber use your model to improve our product?

The following is a flowchart of the workflow of this part of the take
home challenge.

\begin{figure}
\centering
\includegraphics{Workflow.png}
\caption{UBER Take Home Challenge - Workflow}
\end{figure}

    \section{0- Domain Research}\label{domain-research}

Before jumping into the problem, we need to perform some domain
research. The main question here is, how one can become an Uber Driver?

Looking at \href{http://www.uberdriverusa.com/}{Uber webpage} on how to
become an Uber driver, we see several requirements. Those importan for
us are:

" \textbf{7. You must pass driving record check and background check.}

In addition to the above requirements, you must also pass a driving
record check and background check. Basically, Uber will require you to
have no major violations in the past 3 years, no drug-related driving
violations in the last 7 years and no extreme infractions in the last 7
years.

In the background check, within the past seven years, you will have no
violent crimes, no theft, no sexual offenses, no property damage, no
drug-related offenses and no felonies committed.

You might not have the cleanest driving record or background check, but
that cannot prevent you from becoming an Uber driver.

"

    \section{1- Problem Formulation:}\label{problem-formulation}

This section is dedicated to classifying whether a user becomes an Uber
driver using the provided data. The purpose of the author is to find
patterns that demonstrates the drivers' behaviour following their signup
with Uber. There are two higher level business question that need to be
answered:

Does the driver become an Uber driver? This is a "Yes"/"No" question,
since we are interested to know which \textbf{class} of a certain target
each driver is a part of. For problems related to classes, we need to
train classifiers. For this question, our index is the ID of the driver,
and our target is wheather he become a driver or not yet.

The rest of the notebook for this task is as follows:

\texttt{1-1-\ Required\ Packages}

\texttt{1-2-\ Defining\ the\ Choice\ of\ Metrics}

\texttt{1-3-\ Choice\ of\ Train,\ Holdout\ Split}

\texttt{2-\ Exploratory\ Data\ Analysis}

\texttt{2-1-\ Data\ Shape,\ Size,\ Columns,\ Index,\ Types,}

\texttt{2-2-\ Working\ on\ Null\ Values,\ Duplicates,}

\texttt{3-\ Visual\ Exploratory\ Data\ Analysis}

\texttt{3-1-\ Numerical\ Features}

\texttt{3-2-\ Categorical\ Features}

\texttt{3-3-\ Time-series\ Features}

\texttt{4-\ Data\ Preconditioning}

\texttt{4-1-\ Outlier\ Analysis}

\texttt{4-2-\ Train-\ Holdout\ split}

\texttt{5-\ Feature\ Engineering}

\texttt{5-1-\ Numerical\ Features,\ and\ their\ Statistical\ Significance}

\texttt{5-2-\ Categorical\ Features,\ and\ their\ Statistical\ Significance}

\texttt{5-3-\ Time\ Series\ Features,\ and\ their\ Statistical\ Significance}

\texttt{5-4-\ Model\ Preprocessing\ (Standard\ Scaler)}

\texttt{5-4-\ Feature\ Selection\ and\ Feature\ Imporatnce\ (PCA,\ RF)}

\texttt{6-\ Modelling}

\texttt{6-1-\ Simple\ Model\ to\ Check\ Bias/Variance}

\texttt{6-2-\ SVM/KNN\ and\ cross\ validation}

\texttt{6-3-\ Random\ Forest\ as\ a\ Bagging\ Technique\ /Hyper\ Parameter\ Tunning}

\texttt{6-4-\ Gradient\ Boosted\ Trees\ as\ a\ Boosting\ Technique}

\texttt{7-\ Discussion\ and\ Suggestions}

    \subsection{1-1- Required Packages:}\label{required-packages}

\begin{itemize}
\item
  pandas for data manipulation,
\item
  matplotlib, Seaborn for data visualization,
\item
  scipy for statistical Analysis
\item
  sikit-learn for statistical modelling
\item
  numpy for array manipulation
\item
  datetime for time series analysis
\item
  xgboost to perfom modelling using gradient boosted trees
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} inline plots with seaborn style}
        \PY{k+kn}{import} \PY{n+nn}{scipy}
        \PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k}{import} \PY{n}{stats}
        \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{import} \PY{n}{chi2\PYZus{}contingency}
        \PY{k+kn}{import} \PY{n+nn}{sklearn}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{ExtraTreesClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
        \PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{mosaicplot} \PY{k}{import} \PY{n}{mosaic}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{,}\PY{n}{roc\PYZus{}curve}\PY{p}{,} \PY{n}{auc}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
        \PY{k+kn}{from} \PY{n+nn}{datetime} \PY{k}{import} \PY{n}{datetime}
        \PY{k+kn}{from} \PY{n+nn}{datetime} \PY{k}{import} \PY{n}{timedelta}
        \PY{k+kn}{from} \PY{n+nn}{xgboost} \PY{k}{import} \PY{n}{XGBClassifier}
        \PY{k+kn}{import} \PY{n+nn}{xgboost} \PY{k}{as} \PY{n+nn}{xgb}
        \PY{k+kn}{from} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{import} \PY{n}{rcParams}
        \PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}
\end{Verbatim}


    \subsection{1-2- Choice of metrics:}\label{choice-of-metrics}

A very important question we need to answer at the beginning is, what do
we call a good model? For classification, there are several
\href{https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/}{metrics}
we can use, namely accuracy, F-1 Score and Area Under Curve, accuracy.
Before knowing which one to use, we need to take a cook look at data and
see how classes are distributed?

So let's import the data:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{class\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ds\PYZus{}challenge\PYZus{}v2\PYZus{}data.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}    city\_name    signup\_os signup\_channel signup\_date bgc\_date  \textbackslash{}
        id                                                              
        1     Strark      ios web           Paid      1/2/16      NaN   
        2     Strark      windows           Paid     1/21/16      NaN   
        3    Wrouver      windows        Organic     1/11/16  1/11/16   
        4     Berton  android web       Referral     1/29/16   2/3/16   
        5     Strark  android web       Referral     1/10/16  1/25/16   
        
           vehicle\_added\_date vehicle\_make vehicle\_model  vehicle\_year  \textbackslash{}
        id                                                               
        1                 NaN          NaN           NaN           NaN   
        2                 NaN          NaN           NaN           NaN   
        3                 NaN          NaN           NaN           NaN   
        4              2/3/16       Toyota       Corolla        2016.0   
        5             1/26/16      Hyundai        Sonata        2016.0   
        
           first\_completed\_date  
        id                       
        1                   NaN  
        2                   NaN  
        3                   NaN  
        4                2/3/16  
        5                   NaN  
\end{Verbatim}
            
    \begin{quote}
We need to set up our target, which is where the
\texttt{first\_completed\_date} is available (which means that the
driver actually started the service).
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} here I take a look at the first\PYZus{}completed\PYZus{}date and generate a \PYZdq{}1\PYZdq{} if data exists in that column}
        \PY{c+c1}{\PYZsh{}float allows easier divsion}
        \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{first\PYZus{}completed\PYZus{}date}\PY{o}{.}\PY{n}{notnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:}    city\_name    signup\_os signup\_channel signup\_date bgc\_date  \textbackslash{}
        id                                                              
        1     Strark      ios web           Paid      1/2/16      NaN   
        2     Strark      windows           Paid     1/21/16      NaN   
        3    Wrouver      windows        Organic     1/11/16  1/11/16   
        4     Berton  android web       Referral     1/29/16   2/3/16   
        5     Strark  android web       Referral     1/10/16  1/25/16   
        
           vehicle\_added\_date vehicle\_make vehicle\_model  vehicle\_year  \textbackslash{}
        id                                                               
        1                 NaN          NaN           NaN           NaN   
        2                 NaN          NaN           NaN           NaN   
        3                 NaN          NaN           NaN           NaN   
        4              2/3/16       Toyota       Corolla        2016.0   
        5             1/26/16      Hyundai        Sonata        2016.0   
        
           first\_completed\_date  target  
        id                               
        1                   NaN     0.0  
        2                   NaN     0.0  
        3                   NaN     0.0  
        4                2/3/16     1.0  
        5                   NaN     0.0  
\end{Verbatim}
            
    \begin{quote}
Let's see how is the class ratio (base rate). In other words, how much
of the data belongs to the users who became a driver?
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{base\PYZus{}rate} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The class base rate is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{base\PYZus{}rate}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The class base rate is 11.22.

    \end{Verbatim}

    \begin{quote}
The base rate is about 11.22 \%. We, therefore, cannot get that much
insight by evaluating the model using \texttt{accuracy} metric. The
reason is that accuracy loses its meaning for skewed target data. We
will, therefore, use \texttt{area\ under\ curve\ (AUC)} as the metric we
want to maximize. This parameter can demonstrate the method efficiency
while not being affected by the class skewness.
\end{quote}

    \subsection{1-3- Choice of Train/Holdout
Split}\label{choice-of-trainholdout-split}

We need to see, not only, how our technique work on the data we have
available, but also how can it be generalized to the real world
situation. This is best done by testing the model on a holdout dataset.
The relationship between the hold-out set( data we keep intact untill
the last moment), and the train data, should mimic the relationship
between the whole data and real-world situation we will eventually be
tested on.

We have two options for train test split, (1) time-wise split, (2)
random split.

We need to take a look at the target variable to see how it changes over
indexes, and make sure that our hold-out set is a random part of the
data. Let's see how target changes over time:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} we need to take a sample of data since we need the data to be representable}
        \PY{c+c1}{\PYZsh{} we have to change target data type from boolean to integer}
        \PY{n}{target\PYZus{}progression} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
        \PY{n}{target\PYZus{}progression}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scatter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} Text(0,0.5,'target value')
\end{Verbatim}
            
    
    \begin{verbatim}
<matplotlib.figure.Figure at 0x1d20d5712e8>
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
By looking at this target, we can say that a basic random train test
split is enough. We, moreover, don't need to stratify our split since
the classess are not extremely skewed. We perform the train holdout
split in section \texttt{4-2}.
\end{quote}

    \section{2- Exploratory Data Analysis
(EDA)}\label{exploratory-data-analysis-eda}

EDA is one of the most important part of a successful data mining
project. It allows dealing with nuisance data, and also allows getting a
feel for the data at hand.

    \subsection{2-1- Data Shape, Size, Columns, Index,
Types}\label{data-shape-size-columns-index-types}

Let's see how the data is shaped, and look at the available data types
and see if we need to take care of anything.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data has }\PY{l+s+si}{\PYZob{}0\PYZcb{}}\PY{l+s+s2}{ rows and }\PY{l+s+si}{\PYZob{}1\PYZcb{}}\PY{l+s+s2}{ columns.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Data has 54681 rows and 11 columns.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:}    city\_name    signup\_os signup\_channel signup\_date bgc\_date  \textbackslash{}
        id                                                              
        1     Strark      ios web           Paid      1/2/16      NaN   
        2     Strark      windows           Paid     1/21/16      NaN   
        3    Wrouver      windows        Organic     1/11/16  1/11/16   
        4     Berton  android web       Referral     1/29/16   2/3/16   
        5     Strark  android web       Referral     1/10/16  1/25/16   
        
           vehicle\_added\_date vehicle\_make vehicle\_model  vehicle\_year  \textbackslash{}
        id                                                               
        1                 NaN          NaN           NaN           NaN   
        2                 NaN          NaN           NaN           NaN   
        3                 NaN          NaN           NaN           NaN   
        4              2/3/16       Toyota       Corolla        2016.0   
        5             1/26/16      Hyundai        Sonata        2016.0   
        
           first\_completed\_date  target  
        id                               
        1                   NaN     0.0  
        2                   NaN     0.0  
        3                   NaN     0.0  
        4                2/3/16     1.0  
        5                   NaN     0.0  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 54681 entries, 1 to 54681
Data columns (total 11 columns):
city\_name               54681 non-null object
signup\_os               47824 non-null object
signup\_channel          54681 non-null object
signup\_date             54681 non-null object
bgc\_date                32743 non-null object
vehicle\_added\_date      13134 non-null object
vehicle\_make            13223 non-null object
vehicle\_model           13223 non-null object
vehicle\_year            13223 non-null float64
first\_completed\_date    6137 non-null object
target                  54681 non-null float64
dtypes: float64(2), object(9)
memory usage: 5.0+ MB

    \end{Verbatim}

    \begin{quote}
We have 11 columns, and one index column. From these, there are 4
date\_time features, 5 categorical, and one numerical fatures. ** A
large portion of several columns, e.g., vehicle\_added\_date, is
missing. We might find value in them, as they might correlate with the
target. It will be taken care of in the next section**
\end{quote}

Next, I like to check the distribution of each feature. For that, one
can use \texttt{describe} method. But before that, I like to convert the
date columns into date\_time data type. This allows time series
manipulation that we highly rely on later.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{date\PYZus{}columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bgc\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
        \PY{k}{for} \PY{n}{date\PYZus{}column} \PY{o+ow}{in} \PY{n}{date\PYZus{}columns}\PY{p}{:}
            \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{date\PYZus{}column}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{date\PYZus{}column}\PY{p}{]}\PY{p}{,}\PY{n}{infer\PYZus{}datetime\PYZus{}format}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:}        vehicle\_year        target
         count  13223.000000  54681.000000
         mean    2010.568025      0.112233
         std       35.219184      0.315656
         min        0.000000      0.000000
         25\%     2008.000000      0.000000
         50\%     2013.000000      0.000000
         75\%     2015.000000      0.000000
         max     2017.000000      1.000000
\end{Verbatim}
            
    \begin{quote}
The only numeric column, vehicle year, demonstrates that on average cars
are from 2010, however, the median is much higher at 2013. Moreover, we
see that newest cars are from 2017 and oldest are from 1995. We see that
there are some cars with vehicle year = 0. These are some peculiar cases
we need to deal with. . Let's see in case "0" is outlier data, what
would be the vehicle\_year distribution?
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:}        vehicle\_year        target
         count  13219.000000  54677.000000
         mean    2011.176413      0.112241
         std        4.135149      0.315666
         min     1995.000000      0.000000
         25\%     2008.000000      0.000000
         50\%     2013.000000      0.000000
         75\%     2015.000000      0.000000
         max     2017.000000      1.000000
\end{Verbatim}
            
    \begin{quote}
We can still see that the mean is smaller, and therefore, we can say
that the vehicle year is left skewed and in general newer cars tend to
apply more to become an Uber driver. \textbf{This gives an insight that
maybe younger people are more interested in becoming Uber driver. This
calls for getting an additional data from the users, say birthdate.}
\end{quote}

By providing an argument inside describe method, we can get the
distribution of categorical variables.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:}        city\_name signup\_os signup\_channel vehicle\_make vehicle\_model
         count      54681     47824          54681        13223         13223
         unique         3         5              3           46           368
         top       Strark   ios web           Paid       Toyota         Civic
         freq       29557     16632          23938         3219           689
\end{Verbatim}
            
    \begin{quote}
We observe that: - there are three cities in this data set and Strark is
highest repeated. - same goes to signu\_os, with 5 classes and ios\_web
being most popular. - there are also three sign up channel and most
repeated one is Paid. - there are 46 vehicle make. We need to dive more
into this feature as it might be a good candidate for categorical
feature. - there are 368 vehicle makes, this for sure is a large number
of categories. We need to take a deeper look at it in the feature
engineering section to see if we can get relative information from it.
\end{quote}

Let's see how the time-series features are distributed?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{date\PYZus{}columns}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:}                 signup\_date             bgc\_date   vehicle\_added\_date  \textbackslash{}
         count                 54681                32743                13134   
         unique                   30                   74                   78   
         top     2016-01-05 00:00:00  2016-01-29 00:00:00  2016-01-26 00:00:00   
         freq                   2489                 1119                  377   
         first   2016-01-01 00:00:00  2016-01-01 00:00:00  2016-01-01 00:00:00   
         last    2016-01-30 00:00:00  2016-03-25 00:00:00  2016-03-26 00:00:00   
         
                first\_completed\_date  
         count                  6137  
         unique                   57  
         top     2016-01-23 00:00:00  
         freq                    257  
         first   2016-01-04 00:00:00  
         last    2016-02-29 00:00:00  
\end{Verbatim}
            
    \begin{quote}
The drivers signed up through January and their backgrounds are checked
by March 25th. Moreover, the drivers uploaded their information by March
26th. Moreover, the first completed date finishes by the end of
February. This means that none of the drivers who approved their
background check and uploaded their information after the end of
February has actually become a driver. This calls for engineering a
feature, that captures this late submissions.
\end{quote}

    \subsection{2-2- Working on Null Values,
Duplicates}\label{working-on-null-values-duplicates}

First steps in cleaning the data is dealing with null-values and
duplicates. First let's deal with the duplicate rows. We don't want to
over train our model by data points that carry exactly the same
information. Let's see how many duplicates do we have?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{number\PYZus{}of\PYZus{}duplicates} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
         \PY{n}{number\PYZus{}of\PYZus{}rows} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s2}{f the observations are duplicates.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{number\PYZus{}of\PYZus{}duplicates}\PY{o}{/}\PY{n}{number\PYZus{}of\PYZus{}rows}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
48.4\% of the observations are duplicates.

    \end{Verbatim}

    \begin{quote}
We have "26467" duplicate rows, which is quite large, it is almost 50\%
of the data. Are we throwing them out? No!! We assume that the only
parameter that can demonstrate that an observation is duplicate is the
ID. Having small number of features as we have here, this number
duplicates can be normal. It is actually promissing that there are this
much duplicates, it shows that some patterns are available in data.
Let's see how many duplicates are avaialble including the ID?
\end{quote}

\textbf{Caveat: we made an assumption about the origin of duplicates.
However, we need to perform more research. Are they really duplicates?
Is there any bug in our system that same customer is trying to submit
several times? The main reason for this caution is that once the
duplicates are dropped the mean target value doubles. In other words
most duplicates belong to the non dirver class}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{base\PYZus{}rate} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The class base rate is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{base\PYZus{}rate}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{base\PYZus{}rate\PYZus{}dropped\PYZus{}duplicates} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The class base rate when duplicates are dropped is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{base\PYZus{}rate\PYZus{}dropped\PYZus{}duplicates}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The class base rate is 11.22.
The class base rate when duplicates are dropped is 21.75.

    \end{Verbatim}

    \begin{quote}
Since the ID of the observations ara distinct, we assume that the
duplicates are merely the results of small number of features.
\end{quote}

Now let's take a look at the missing values. I believe the power of any
model on this data set, depends on good utilization and understanding of
missing values an their meaning. There are four options when you get to
NaN values:

\begin{itemize}
\item
  Dropping observations with nan values (when nans are simple
  meaningless coincindences)
\item
  Deleting features with nan values (when there are too many nans and
  they seam to be not very explnatory)
\item
  Imputing nans as a with new values, e.g. new class for categorical, or
  a very far number for numericals when we can't throw the nans out.
\item
  Generate a new feature from the column that has nan values, where the
  feature is one when there is a nan and 0 otherwise. This is in the
  cases when nan values carry some meaning.
\end{itemize}

Let's agin quickly look at he missing values in each column.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 54681 entries, 1 to 54681
Data columns (total 11 columns):
city\_name               54681 non-null object
signup\_os               47824 non-null object
signup\_channel          54681 non-null object
signup\_date             54681 non-null datetime64[ns]
bgc\_date                32743 non-null datetime64[ns]
vehicle\_added\_date      13134 non-null datetime64[ns]
vehicle\_make            13223 non-null object
vehicle\_model           13223 non-null object
vehicle\_year            13223 non-null float64
first\_completed\_date    6137 non-null datetime64[ns]
target                  54681 non-null float64
dtypes: datetime64[ns](4), float64(2), object(5)
memory usage: 5.0+ MB

    \end{Verbatim}

    \begin{quote}
The smallest number of null values is in coluumn \texttt{signup\_os}
which is 12.5\% \((=\frac{54681-47824}{54681}\%)\) of the total number
of available observations. We highly prefer not losing this much
information, if it can be avoided. For that let's see if we can impute,
or use the nan-value of each column.
\end{quote}

First, let's take a look at \texttt{signup\_os} column.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{signup\PYZus{}os}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} create a cateogory for missing}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} ios web        16632
         android web    14944
         missing         6857
         windows         6776
         mac             5824
         other           3648
         Name: signup\_os, dtype: int64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}os}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}os}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} signup\_os
         android web    0.097297
         ios web        0.131734
         mac            0.162775
         missing        0.021584
         other          0.136513
         windows        0.132527
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{quote}
We can see that the imputed \texttt{missing} category has much smaller
driver approval rate that any other category. Therefore, it might be a
very meaningful feature. In other words, there might be some conditions
related to this missing value. For example, there might be some specific
device that doesn't report the operating system. We will keep this
\texttt{nan} value as a new category in data. Later in section 5, we
will check the statistical significance of this cateogry.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}os}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}os}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Let's move to \texttt{bgc\_date} column. Taking a look at \texttt{info}
method used above, we see that 40\% \((=\frac{54681-32743}{54681}\%)\)
of the rows are missing in \texttt{bgc\_date}. When there is no data on
\texttt{bgc\_date}, it might mean that the background check has not been
permitted, not necessarily the data is missing. We can check such
hypothesis by checking the mean value of the bgc\_date for missign and
non\_missing values.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bcg\PYZus{}checked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{bgc\PYZus{}date}\PY{o}{.}\PY{n}{notnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Yes}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{x} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bcg\PYZus{}checked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bcg\PYZus{}checked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{aggregate}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}programdata\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:1: FutureWarning: using a dict on a Series for aggregation
is deprecated and will be removed in a future version
  """Entry point for launching an IPython kernel.

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:}                  mean  count
         bcg\_checked                 
         No           0.006974  21938
         Yes          0.182757  32743
\end{Verbatim}
            
    \begin{quote}
Our hyposthesis seems to be correct. For drivers with data missing on
background, only 0.007 become Uber driver, which is 6\% of all the
approved. Therefore, we can easily say that when the data is missing on
the background, the driver has not been approved/recieved/applied
his/her backrgound info to move forward. Therefore, we can say that we
can generate a new feature based on this feature which highly explains
the variablity. Let's prove this by checking the significance of this
observation.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{missing} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{bcg\PYZus{}checked}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{target}
         \PY{n}{rest} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{bcg\PYZus{}checked}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{target}
         \PY{n}{stats}\PY{o}{.}\PY{n}{ttest\PYZus{}ind}\PY{p}{(}\PY{n}{rest}\PY{p}{,} \PY{n}{missing}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} Ttest\_indResult(statistic=-66.34529950072451, pvalue=0.0)
\end{Verbatim}
            
    \begin{quote}
We see that, statistically speaking, the new feature is capable of
describing the variance in the target data.
\end{quote}

Not Let's move to the next features.

vehicle\_added\_date has 13134 non-null datetime64{[}ns{]} which is we
see that 75\% \((=\frac{54681-13134}{54681}\%)\) of the rows.

We will write a piece of code that calculates the significance of
missing values.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k}{def} \PY{n+nf}{missing\PYZus{}value\PYZus{}significance}\PY{p}{(}\PY{n}{dataframe}\PY{p}{,} \PY{n}{column}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Checking if missing value in }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n}{column}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ is significant.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean value for each class level.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{dataframe}\PY{p}{[}\PY{n}{column}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataframe}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{o}{.}\PY{n}{notnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Yes}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{x} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{dataframe}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{column}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{n}{column}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{aggregate}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{missing} \PY{o}{=} \PY{n}{dataframe}\PY{p}{[}\PY{n}{dataframe}\PY{p}{[}\PY{n}{column}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{target}
             \PY{n}{rest} \PY{o}{=} \PY{n}{dataframe}\PY{p}{[}\PY{n}{dataframe}\PY{p}{[}\PY{n}{column}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{target}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Checking if p\PYZhy{}value is significant.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{stats}\PY{o}{.}\PY{n}{ttest\PYZus{}ind}\PY{p}{(}\PY{n}{rest}\PY{p}{,} \PY{n}{missing}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{missing\PYZus{}value\PYZus{}significance}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Checking if missing value in vehicle\_added\_date is significant.

Mean value for each class level.

                                  mean  count
vehicle\_added\_date\_available                 
No                            0.006378  41547
Yes                           0.447084  13134
Checking if p-value is significant.

Ttest\_indResult(statistic=-173.76141725983868, pvalue=0.0)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}programdata\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:5: FutureWarning: using a dict on a Series for aggregation
is deprecated and will be removed in a future version
  """

    \end{Verbatim}

    \begin{quote}
The missing values on the vehicle added date are also very informative
and we will keep them. \textbf{UBER might be able to send follow up
emails to the registrators and ask them to upload their information.
User might commit more to the program by uploading their information.
Moreover, streamling the process might make it easier for the users to
become an Uber driver.}
\end{quote}

Now, let's move to the next three columns, they have the same number of
null objects. Intuitively, we guess the null values are from the same
rows. Let's check that.

vehicle\_make 13223 non-null object

vehicle\_model 13223 non-null object

vehicle\_year 13223 non-null float64

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{info\PYZus{}column} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{info\PYZus{}column}\PY{p}{]}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 13223 entries, 4 to 54305
Data columns (total 3 columns):
vehicle\_make     13223 non-null object
vehicle\_model    13223 non-null object
vehicle\_year     13223 non-null float64
dtypes: float64(1), object(2)
memory usage: 413.2+ KB

    \end{Verbatim}

    \begin{quote}
Yes! We see that hey are from the same observations. Therefore, we need
to only analyse one of them and see if we can generate some information
from it?
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{missing\PYZus{}value\PYZus{}significance}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Checking if missing value in vehicle\_make is significant.

Mean value for each class level.

                            mean  count
vehicle\_make\_available                 
No                      0.006368  41458
Yes                     0.444150  13223
Checking if p-value is significant.

Ttest\_indResult(statistic=-172.59479452381981, pvalue=0.0)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}programdata\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:5: FutureWarning: using a dict on a Series for aggregation
is deprecated and will be removed in a future version
  """

    \end{Verbatim}

    \begin{quote}
Yes. We see that in this feature also the missing values carries
information. We will keep this.
\end{quote}

    \section{3- Visual Exploratory Data
Analysis}\label{visual-exploratory-data-analysis}

Next, we need to visualize and see some information through
visualization.

    \subsection{3-1- Numerical Features}\label{numerical-features}

We only have one numerical faeture, let's ee how it looks like.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year vehicle built}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} Text(0.5,0,'Year vehicle built')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We saw in section 2-1 that this column has some 0 values which we are
not sure what they mean. For now we drop them and see if they are
important.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{\PYZam{}}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                  \PY{n}{normed} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle age for Uber drivers histogram}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{\PYZam{}}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle age for Uber drivers kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year vehicle built}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histogram and Kernel Density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{mean\PYZus{}yes} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{\PYZam{}}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean = }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{int}\PY{p}{(}\PY{n}{mean\PYZus{}yes}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{mean\PYZus{}yes}\PY{p}{,}\PY{l+m+mf}{0.155}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{\PYZam{}}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                  \PY{n}{normed} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle age for not yet Uber drivers histogram}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{\PYZam{}}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle age for not yet Uber drivers drivers kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year vehicle built}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histogram and Kernel Density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{mean\PYZus{}no} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{\PYZam{}}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean = }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{int}\PY{p}{(}\PY{n}{mean\PYZus{}no}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{mean\PYZus{}no}\PY{o}{+}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{0.145}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
Distribution is slightly different. We can see that the vehicles woe
become uber driver are about 1.5 years younger than the one who hasn't
yet become a driver.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{aggregate}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}programdata\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:1: FutureWarning: using a dict on a Series for aggregation
is deprecated and will be removed in a future version
  """Entry point for launching an IPython kernel.

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}29}]:}                mean  count
         target                    
         0.0     2009.863129   7350
         1.0     2011.450196   5873
\end{Verbatim}
            
    \begin{quote}
This feature seems to be very promissing for generating a feature.
\end{quote}

    \subsection{3-2- Categorical Features}\label{categorical-features}

We have a few categorical features. Main three are:

\begin{itemize}
\tightlist
\item
  city\_name
\item
  signup\_os
\item
  signup\_channel
\end{itemize}

(The rests are added through our missing value analysis. You can ignore
them here).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:}        city\_name signup\_os signup\_channel vehicle\_make vehicle\_model  \textbackslash{}
         count      54681     54681          54681        13223         13223   
         unique         3         6              3           46           368   
         top       Strark   ios web           Paid       Toyota         Civic   
         freq       29557     16632          23938         3219           689   
         
                bcg\_checked vehicle\_added\_date\_available vehicle\_make\_available  
         count        54681                        54681                  54681  
         unique           2                            2                      2  
         top            Yes                           No                     No  
         freq         32743                        41547                  41458  
\end{Verbatim}
            
    Mosaic plot gives a very good picture of the categorical data when the
number of categories are small and the target is also categorical (for
classication tasks).

Let's see the first feature (\texttt{city\_name}).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{mosaic}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{city\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{title} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{City name categories}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{n}{gap}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} Text(0,0.5,'target')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_71_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
It seems city of has lower Uber driver ratio. Let's quickly look at the
Chi-square test on these features and see if they explain something.
\end{quote}

Chi-square test runs on contingency table which correlates categorical
variables to each other.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{observed\PYZus{}city\PYZus{}contingency} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{p}{,}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{city\PYZus{}name}\PY{p}{)}
         \PY{n}{observed\PYZus{}city\PYZus{}contingency}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}32}]:} city\_name  Berton  Strark  Wrouver
         target                            
         0.0         17680   26318     4546
         1.0          2437    3239      461
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{chi2\PYZus{}contingency}\PY{p}{(}\PY{n}{observed\PYZus{}city\PYZus{}contingency}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The p\PYZhy{}value for the city feature is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{p}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The p-value for the city feature is 4.2967762262368374e-09.

    \end{Verbatim}

    \begin{quote}
The city name feature is informative as its p-value is much smaller than
type-1 error of significance 0.05.
\end{quote}

Let's move to the next categorical feature, \texttt{signup\_os}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{mosaic}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}os}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{title} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Operating system categories}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}34}]:} Text(0,0.5,'target')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_76_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
Web site (through ios and android) is the main interface for
registration. Moreover, missing values seem to have much smaller chance
of final registration.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{observed\PYZus{}os\PYZus{}contingency} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{p}{,}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{signup\PYZus{}os}\PY{p}{)}
         \PY{n}{observed\PYZus{}os\PYZus{}contingency}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}35}]:} signup\_os  android web  ios web   mac  missing  other  windows
         target                                                        
         0.0              13490    14441  4876     6709   3150     5878
         1.0               1454     2191   948      148    498      898
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{chi2\PYZus{}contingency}\PY{p}{(}\PY{n}{observed\PYZus{}os\PYZus{}contingency}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The p\PYZhy{}value for the operating system feature is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{p}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The p-value for the operating system feature is 6.119148515923839e-184.

    \end{Verbatim}

    \begin{quote}
Absolutely statistically signiticant.
\end{quote}

Let's move to the next feature, \texttt{signup\_channel}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{mosaic}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}channel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{title} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Channel categories}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} Text(0,0.5,'target')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
Referals are most loyals and the paid group are least. Let's see if
there is statistical significance.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{observed\PYZus{}channel\PYZus{}contingency} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{target}\PY{p}{,}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{signup\PYZus{}channel}\PY{p}{)}
         \PY{n}{observed\PYZus{}channel\PYZus{}contingency}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:} signup\_channel  Organic   Paid  Referral
         target                                  
         0.0               12217  22456     13871
         1.0                1210   1482      3445
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{chi2\PYZus{}contingency}\PY{p}{(}\PY{n}{observed\PYZus{}channel\PYZus{}contingency}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The p\PYZhy{}value for the channel feature is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{p}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The p-value for the channel feature is 0.0.

    \end{Verbatim}

    Before going to the next phase, we need to take care of a categorical
variable, \textbf{\emph{vehicle\_model}}. There are vehicles in that
data set that are really rare, and might actually result in overfitting,
and also possibly impossible train\_test split. What I will be doing, is
to categorize rare levels of the categories into one single group. If
number of vehicles in that class is less than 40, we call it rare.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{vehicle\PYZus{}model\PYZus{}count\PYZus{}dict} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}
             \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rare}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{vehicle\PYZus{}model\PYZus{}count\PYZus{}dict}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mi}{30} \PY{k}{else} \PY{n}{x}\PY{p}{)}
\end{Verbatim}


    \subsection{3-3- Time-series Features}\label{time-series-features}

Now let's see how target changes over time with the signup date. What we
want to see here is how many registeration is performed daily and from
them, how much of the portion has been successfull.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{registeration\PYZus{}count}\PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{resample}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
         \PY{n}{activation\PYZus{}mean} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{resample}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         \PY{n}{successfull\PYZus{}registration} \PY{o}{=} \PY{n}{activation\PYZus{}mean}\PY{o}{*}\PY{n}{registeration\PYZus{}count}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{211}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{successfull\PYZus{}registration}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{successfull\PYZus{}registration}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gold}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{successful registration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{registeration\PYZus{}count}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{registeration\PYZus{}count}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saddlebrown}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total registration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{registeration\PYZus{}count}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{successfull\PYZus{}registration}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time of registration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} of registration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{212}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{successfull\PYZus{}registration}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{activation\PYZus{}mean}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time of registration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
From the left top we can see that there is a weekley periodicity in the
number of successful registration. It is evident that the number of
registration decreases over the weekend (e.g. 2016-01-10 is Sunday), and
it goes up in the beginning of the week.
\end{quote}

\begin{quote}
From the bottom figure, we see that in the same way, the target ratio
(of drivers to non-drivers) also decreases over weekends. This means
that in general less drivers start their work over the weekend.
\end{quote}

Let's now see how the response variable changes over time. In other
words, how many daily activation do we have?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{fig1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{211}\PY{p}{)}
         \PY{n}{activation\PYZus{}count\PYZus{}daily} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{resample}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{activation\PYZus{}count\PYZus{}daily}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{activation\PYZus{}count\PYZus{}daily}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dashed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{daily acitivation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} of activation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time (Day)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{212}\PY{p}{)}
         \PY{n}{activation\PYZus{}count\PYZus{}weekly} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{resample}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{activation\PYZus{}count\PYZus{}weekly}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{activation\PYZus{}count\PYZus{}weekly}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dashed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekly acitivation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} of activation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time (Week)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
We have a meaningful variation in the daily response of the customers
who registered in January. End of the January is the peak and as we
progress toward the end of February it highly decreases.
\end{quote}

    \section{4- Data Preconditioning:}\label{data-preconditioning}

There are a couple of preconditioning steps we need to take care before
going into feature engineering. First we need to take care of outliers,
and then perform a train-holdout study to assure correct generalization.

    \section{4-1- Outlier Analysis}\label{outlier-analysis}

Outliers analysis is a very important preprocessing step. We need to
analyze if the outliers carry any information. Througout this study, the
only feature I notice that has outlier, is the vehicle year that has
some zero values. Let's quickly look at them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{There are }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ outliers in vehicle\PYZus{}year feature.}\PY{l+s+s2}{\PYZdq{}}
              \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 4 outliers in vehicle\_year feature.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:}       city\_name signup\_os signup\_channel signup\_date   bgc\_date  \textbackslash{}
         id                                                                
         20018    Berton     other        Organic  2016-01-13        NaT   
         44838    Strark       mac        Organic  2016-01-17 2016-01-21   
         48129    Strark   ios web           Paid  2016-01-06 2016-01-09   
         49607    Strark       mac        Organic  2016-01-13 2016-01-21   
         
               vehicle\_added\_date vehicle\_make vehicle\_model  vehicle\_year  \textbackslash{}
         id                                                                  
         20018         2016-02-29         Bike          rare           0.0   
         44838         2016-01-26    Chevrolet          rare           0.0   
         48129         2016-01-10         Ford          rare           0.0   
         49607         2016-01-24       Subaru       Impreza           0.0   
         
               first\_completed\_date  target bcg\_checked vehicle\_added\_date\_available  \textbackslash{}
         id                                                                            
         20018                  NaT     0.0          No                          Yes   
         44838                  NaT     0.0         Yes                          Yes   
         48129                  NaT     0.0         Yes                          Yes   
         49607                  NaT     0.0         Yes                          Yes   
         
               vehicle\_make\_available  
         id                            
         20018                    Yes  
         44838                    Yes  
         48129                    Yes  
         49607                    Yes  
\end{Verbatim}
            
    There are only four outliers, and it is safe to drop them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{class\PYZus{}data} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{vehicle\PYZus{}year}\PY{o}{!=}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


    \subsection{4-2- Train- Holdout split}\label{train--holdout-split}

Before, starting to create new feature from the data, we need to take a
part of data out so that this data is \textbf{blinded} to all of our
feature engineering process.

Train-test split from sklearn drops the column names and converts the
dataframe to numpy array, we need to write a function that brings it
back to dataframe format.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{class\PYZus{}data} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:}    id city\_name    signup\_os signup\_channel signup\_date   bgc\_date  \textbackslash{}
         0   1    Strark      ios web           Paid  2016-01-02        NaT   
         1   2    Strark      windows           Paid  2016-01-21        NaT   
         2   3   Wrouver      windows        Organic  2016-01-11 2016-01-11   
         3   4    Berton  android web       Referral  2016-01-29 2016-02-03   
         4   5    Strark  android web       Referral  2016-01-10 2016-01-25   
         
           vehicle\_added\_date vehicle\_make vehicle\_model  vehicle\_year  \textbackslash{}
         0                NaT          NaN       missing           NaN   
         1                NaT          NaN       missing           NaN   
         2                NaT          NaN       missing           NaN   
         3         2016-02-03       Toyota       Corolla        2016.0   
         4         2016-01-26      Hyundai        Sonata        2016.0   
         
           first\_completed\_date  target bcg\_checked vehicle\_added\_date\_available  \textbackslash{}
         0                  NaT     0.0          No                           No   
         1                  NaT     0.0          No                           No   
         2                  NaT     0.0         Yes                           No   
         3           2016-02-03     1.0         Yes                          Yes   
         4                  NaT     0.0         Yes                          Yes   
         
           vehicle\_make\_available  
         0                     No  
         1                     No  
         2                     No  
         3                    Yes  
         4                    Yes  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{k}{def} \PY{n+nf}{train\PYZus{}holdout\PYZus{}split}\PY{p}{(}\PY{n}{dataframe}\PY{p}{,} \PY{n}{target\PYZus{}variable}\PY{p}{)}\PY{p}{:}
             \PY{n}{feature\PYZus{}cols} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{dataframe}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{]}
             \PY{n}{feature\PYZus{}cols}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{target\PYZus{}variable}\PY{p}{)}
             \PY{n}{features} \PY{o}{=} \PY{n}{dataframe}\PY{p}{[}\PY{n}{feature\PYZus{}cols}\PY{p}{]}
             \PY{n}{target} \PY{o}{=} \PY{n}{dataframe}\PY{p}{[}\PY{n}{target\PYZus{}variable}\PY{p}{]}
             \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{target}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
             \PY{n}{training} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{hold\PYZus{}out} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{k}{return} \PY{p}{[}\PY{n}{training}\PY{p}{,} \PY{n}{hold\PYZus{}out}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{combined\PYZus{}data} \PY{o}{=} \PY{n}{train\PYZus{}holdout\PYZus{}split}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \section{5- Feature Engineering}\label{feature-engineering}

\subsection{5-1- Numerical Feature, and its Statistical
Significance}\label{numerical-feature-and-its-statistical-significance}

First, I will develop features from our numerical feature,
\texttt{vehicle\ age}. In section \texttt{3-1} we saw that this feature
is significant in explaining variance in the target variable. We will
develop a meaningful feature from this column, named vehicle\_age, and
then perform quantization to assure that the data skewness and outliers
don't affect the modelling.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{2017} \PY{o}{\PYZhy{}} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             
         
         \PY{n}{training\PYZus{}qcut}\PY{p}{,} \PY{n}{bins} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{qcut}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{vehicle\PYZus{}age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{retbins}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{training\PYZus{}qcut}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)} \PY{c+c1}{\PYZsh{} generate a new category for missing data}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{cut}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{vehicle\PYZus{}age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{n}{bins}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{include\PYZus{}lowest}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)} \PY{c+c1}{\PYZsh{} generate a new category for missing data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{n}{observed\PYZus{}age\PYZus{}contingency} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{target}\PY{p}{,}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{vehicle\PYZus{}age}\PY{p}{)}
         \PY{p}{(}\PY{n}{observed\PYZus{}age\PYZus{}contingency}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}52}]:} vehicle\_age   0.0   1.0   2.0    3.0
         target                              
         0.0          1898  1454  1603  27572
         1.0          1599  1215  1117    175
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{chi2\PYZus{}contingency}\PY{p}{(}\PY{n}{observed\PYZus{}age\PYZus{}contingency}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The p\PYZhy{}value for the age category feature is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{p}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The p-value for the age category feature is 0.0.

    \end{Verbatim}

    \begin{quote}
We obviously see that this feature passes the Chi-square test. We will
keep this as a meaningful feature in the final data set.
\end{quote}

We can now drop the vehicle year column.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{dataset}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \subsection{5-2- Categorical Features, and their Statistical
Significance}\label{categorical-features-and-their-statistical-significance}

We have several categorical features that we need to generate insight
from.

We have a few categorical features. Main three are:

\begin{itemize}
\tightlist
\item
  city\_name
\item
  signup\_os
\item
  signup\_channel
\item
  vehicle\_make
\item
  vehicle\_model
\end{itemize}

Before starting to work on the categories, I suggest using an
interaction category between \texttt{signup\_os} and
\texttt{signup\_chanell}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{web\PYZus{}paid\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{signup\PYZus{}channel}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{missing}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{o}{+} \PY{n}{dataset}\PY{o}{.}\PY{n}{signup\PYZus{}os}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{missing}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{web\PYZus{}paid\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{web\PYZus{}paid\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{target}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}56}]:} web\_paid\_interaction
         Organicandroid web     0.074961
         Organicios web         0.091949
         Organicmac             0.153782
         Organicmissing         0.023162
         Organicother           0.114943
         Organicwindows         0.118711
         Paidandroid web        0.038234
         Paidios web            0.058963
         Paidmac                0.088636
         Paidmissing            0.000000
         Paidother              0.086632
         Paidwindows            0.078375
         Referralandroid web    0.217825
         Referralios web        0.259962
         Referralmac            0.330108
         Referralmissing        0.019251
         Referralother          0.283673
         Referralwindows        0.293651
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{quote}
We see some nice observations: We have a group that explains 33\% of
approvals (referral mac), and a group (paid missing) that has
absoloutely zero drivers activation. We keep this feature and check its
significance.
\end{quote}

    I would like to add the country of origin to the vehicle make. Maybe
cars from different origin have different response. I downloaded a
dataset fro
\href{https://github.com/johnashu/datacamp/blob/master/automobiles.csv}{here}
that has the country of origin and MPG for most of the well-known
vehicles. I will aggregate and add the mpg and country of origin to the
dataet.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{n}{cars} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cars.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{cars}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{cars}\PY{o}{.}\PY{n}{name}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{n}{x}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{cars\PYZus{}agg} \PY{o}{=} \PY{n}{cars}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{origin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}} \PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{origin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{dataset}\PY{o}{.}\PY{n}{vehicle\PYZus{}make} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{vehicle\PYZus{}make}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o+ow}{is} \PY{n+nb}{str} \PY{k}{else} \PY{n}{x}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}59}]:}           id city\_name signup\_os signup\_channel signup\_date   bgc\_date  \textbackslash{}
         42793  42795   Wrouver   ios web           Paid  2016-01-23 2016-02-25   
         12766  12767    Strark   windows           Paid  2016-01-18        NaT   
         19140  19141    Strark   ios web           Paid  2016-01-30        NaT   
         49633  49638    Strark   windows           Paid  2016-01-05 2016-01-07   
         45809  45812    Berton   ios web        Organic  2016-01-18        NaT   
         
               vehicle\_added\_date vehicle\_make vehicle\_model first\_completed\_date  \textbackslash{}
         42793         2016-03-01      hyundai        Sonata                  NaT   
         12766                NaT          NaN       missing                  NaT   
         19140                NaT          NaN       missing                  NaT   
         49633         2016-01-12        mazda        MAZDA3           2016-01-23   
         45809                NaT          NaN       missing                  NaT   
         
               bcg\_checked vehicle\_added\_date\_available vehicle\_make\_available  target  \textbackslash{}
         42793         Yes                          Yes                    Yes     0.0   
         12766          No                           No                     No     0.0   
         19140          No                           No                     No     0.0   
         49633         Yes                          Yes                    Yes     1.0   
         45809          No                           No                     No     0.0   
         
                vehicle\_age web\_paid\_interaction  
         42793          0.0          Paidios web  
         12766          3.0          Paidwindows  
         19140          3.0          Paidios web  
         49633          1.0          Paidwindows  
         45809          3.0       Organicios web  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{cars\PYZus{}agg}\PY{p}{,} \PY{n}{on} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{cars\PYZus{}agg}\PY{p}{,} \PY{n}{on} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{mpg} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{mpg}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{value}\PY{o}{=}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{mpg}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{mpg} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{mpg}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{value}\PY{o}{=}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{mpg}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}62}]:}       id city\_name signup\_os signup\_channel signup\_date   bgc\_date  \textbackslash{}
         0  42795   Wrouver   ios web           Paid  2016-01-23 2016-02-25   
         1  12767    Strark   windows           Paid  2016-01-18        NaT   
         2  19141    Strark   ios web           Paid  2016-01-30        NaT   
         3  49638    Strark   windows           Paid  2016-01-05 2016-01-07   
         4  45812    Berton   ios web        Organic  2016-01-18        NaT   
         
           vehicle\_added\_date vehicle\_make vehicle\_model first\_completed\_date  \textbackslash{}
         0         2016-03-01      hyundai        Sonata                  NaT   
         1                NaT          NaN       missing                  NaT   
         2                NaT          NaN       missing                  NaT   
         3         2016-01-12        mazda        MAZDA3           2016-01-23   
         4                NaT          NaN       missing                  NaT   
         
           bcg\_checked vehicle\_added\_date\_available vehicle\_make\_available  target  \textbackslash{}
         0         Yes                          Yes                    Yes     0.0   
         1          No                           No                     No     0.0   
         2          No                           No                     No     0.0   
         3         Yes                          Yes                    Yes     1.0   
         4          No                           No                     No     0.0   
         
            vehicle\_age web\_paid\_interaction origin        mpg  
         0          0.0          Paidios web    NaN  28.318877  
         1          3.0          Paidwindows    NaN  28.318877  
         2          3.0          Paidios web    NaN  28.318877  
         3          1.0          Paidwindows   Asia  30.860000  
         4          3.0       Organicios web    NaN  28.318877  
\end{Verbatim}
            
    In the beginning, we noticed that the duplicate rows tend to belong
highly to the non-activating drivers. Let's see if we can add a column
for taking duplicates into account.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{n}{dup\PYZus{}ids} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{id}\PY{p}{)}
         \PY{n}{dup\PYZus{}ids}
         \PY{k}{for} \PY{n}{data\PYZus{}frame} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{data\PYZus{}frame}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duplicated}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}frame}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{id}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Yes}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{x} \PY{o+ow}{in} \PY{n}{dup\PYZus{}ids} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duplicated}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duplicated}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}63}]:}               target
         duplicated          
         Yes         0.000000
         No          0.216847
\end{Verbatim}
            
    Before going furhter, as we saw the missing values for all these
categories carry information, let's fill all the NAs with 'missing'.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{n}{categorical\PYZus{}features} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
         \PY{n}{categorical\PYZus{}features} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{categorical\PYZus{}features}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}since we include the missing values, we will drop this column}
         \PY{n}{categorical\PYZus{}features}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{categorical\PYZus{}features}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}64}]:} ['city\_name',
          'signup\_os',
          'signup\_channel',
          'vehicle\_make',
          'vehicle\_model',
          'bcg\_checked',
          'vehicle\_added\_date\_available',
          'web\_paid\_interaction',
          'origin',
          'duplicated']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{dataset}\PY{p}{[}\PY{n}{categorical\PYZus{}features}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{n}{categorical\PYZus{}features}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{dataset}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}make\PYZus{}available}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}66}]:}        city\_name signup\_os signup\_channel vehicle\_make vehicle\_model  \textbackslash{}
         count      36633     36633          36633        36633         36633   
         unique         3         6              3           47            90   
         top       Strark   ios web           Paid      missing       missing   
         freq       19828     11053          15889        27747         27747   
         
                bcg\_checked vehicle\_added\_date\_available web\_paid\_interaction   origin  \textbackslash{}
         count        36633                        36633                36633    36633   
         unique           2                            2                   18        4   
         top            Yes                           No      Paidandroid web  missing   
         freq         21956                        27805                 5231    29578   
         
                duplicated  
         count       36633  
         unique          2  
         top            No  
         freq        18935  
\end{Verbatim}
            
    \textbf{We need to convert the categories to number. Because this is
what models understand. This is what called encoding.}

I am suggesting similar encoding for all these categorical variables.
This is what we do:

1- First perform a mean encoding (which means for each categoriy, what
is the mean target variable?).

2- Add a ranker to seperate the classess with close mean ratio.

This highly helps the linear techniques in seperating different groups.
It allows the groups with higher target values to carry higher numbers
and assure a good linearity. Moreover, the classes are better seperated
which assures that trees-based techniques and k-neighbors techniques
have less complication in their modelling.

We perform the encoding on train data and trannsfer the encoding to hold
out.

The following runs through all the categorical variables and results in
label encoded categorical variables.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{n}{df1} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{df2} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{target} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}
         
         \PY{k}{for} \PY{n}{category} \PY{o+ow}{in} \PY{n}{categorical\PYZus{}features}\PY{p}{:}
             \PY{n}{cat\PYZus{}dict} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{df1}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{n}{category}\PY{p}{)}\PY{p}{[}\PY{n}{target}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{1}
             \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{cat\PYZus{}dict}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{cat\PYZus{}dict}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{o}{=} \PY{n}{i}
                 \PY{n}{i} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             \PY{n}{df1}\PY{p}{[}\PY{n}{category}\PY{p}{]} \PY{o}{=} \PY{n}{df1}\PY{p}{[}\PY{n}{category}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{cat\PYZus{}dict}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{)}
             \PY{n}{df2}\PY{p}{[}\PY{n}{category}\PY{p}{]} \PY{o}{=} \PY{n}{df2}\PY{p}{[}\PY{n}{category}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{if} \PY{n}{x} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{cat\PYZus{}dict}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{n}{cat\PYZus{}dict}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    Let's see how the data frames have changed.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}68}]:}       id  city\_name  signup\_os  signup\_channel signup\_date   bgc\_date  \textbackslash{}
         0  42795          1          4               1  2016-01-23 2016-02-25   
         1  12767          2          3               1  2016-01-18        NaT   
         2  19141          2          4               1  2016-01-30        NaT   
         3  49638          2          3               1  2016-01-05 2016-01-07   
         4  45812          3          4               2  2016-01-18        NaT   
         
           vehicle\_added\_date  vehicle\_make  vehicle\_model first\_completed\_date  \textbackslash{}
         0         2016-03-01            22             46                  NaT   
         1                NaT             3              1                  NaT   
         2                NaT             3              1                  NaT   
         3         2016-01-12            31             53           2016-01-23   
         4                NaT             3              1                  NaT   
         
            bcg\_checked  vehicle\_added\_date\_available  target  vehicle\_age  \textbackslash{}
         0            2                             2     0.0          0.0   
         1            1                             1     0.0          3.0   
         2            1                             1     0.0          3.0   
         3            2                             2     1.0          1.0   
         4            1                             1     0.0          3.0   
         
            web\_paid\_interaction  origin        mpg  duplicated  
         0                     5       1  28.318877           2  
         1                     7       1  28.318877           1  
         2                     5       1  28.318877           1  
         3                     7       4  30.860000           2  
         4                    10       1  28.318877           1  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{n}{categorical\PYZus{}features}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Let's see if our categorical features are statistically significant, and
check their p-values.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{k}{def} \PY{n+nf}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{feature}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
             \PY{n}{observed} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{target}\PY{p}{]}\PY{p}{,}\PY{n}{df}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{)}    
             \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{chi2\PYZus{}contingency}\PY{p}{(}\PY{n}{observed}\PY{p}{)}
             \PY{k}{if} \PY{n}{p}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.05}\PY{p}{:}
                 \PY{n}{significance} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{significant}\PY{l+s+s1}{\PYZsq{}}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{significance} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{insignificant}\PY{l+s+s1}{\PYZsq{}}     
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{27}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Chi\PYZhy{}Square test results!!!}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{27}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The p\PYZhy{}value for the correlation bwteern \PYZlt{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{feature}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZgt{} feature and target is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{p}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{This dependance is statistically }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{significance}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{80}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{k}{for} \PY{n}{category} \PY{o+ow}{in} \PY{n}{categorical\PYZus{}features}\PY{p}{:}
             \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{category}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <city\_name> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <signup\_os> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <signup\_channel> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <vehicle\_make> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <vehicle\_model> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <bcg\_checked> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <vehicle\_added\_date\_available> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <web\_paid\_interaction> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <origin> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <duplicated> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <vehicle\_age> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \subsection{5-3- Time Series Features, and their Statistical
Significance}\label{time-series-features-and-their-statistical-significance}

One very important part of this data is the fact that it is time-series.
There are numerous features capable of be extracting from the time
series data.

We will consider the followings:

1- Day of the week (we saw in section 3-3 that the number of
signup/activation decreases over the weekend and increases on Mondays)

2- Time distance since new year in weeks (we see that there is a
meaningful decay in the total number of registration/activation as we
get further away from the new year)

3- Time distance between signup and background check. We will fill the
NAs in the background check with the last day of study. People who get
their background check earlier might be faster in progressing to their
driving.

4- Time distance between signup and info updated.

7- Yesterday backcast. What was yesterday number of successful driver
activations.

8- Late upload: whether the user uploaded his/her information after end
of february.

9- Late background: whether the user gave background permission after
the end of february.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{n}{class\PYZus{}data}\PY{p}{[}\PY{n}{date\PYZus{}columns}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}72}]:}                 signup\_date             bgc\_date   vehicle\_added\_date  \textbackslash{}
         count                 54677                32740                13130   
         unique                   30                   74                   78   
         top     2016-01-05 00:00:00  2016-01-29 00:00:00  2016-01-26 00:00:00   
         freq                   2489                 1119                  376   
         first   2016-01-01 00:00:00  2016-01-01 00:00:00  2016-01-01 00:00:00   
         last    2016-01-30 00:00:00  2016-03-25 00:00:00  2016-03-26 00:00:00   
         
                first\_completed\_date  
         count                  6137  
         unique                   57  
         top     2016-01-23 00:00:00  
         freq                    257  
         first   2016-01-04 00:00:00  
         last    2016-02-29 00:00:00  
\end{Verbatim}
            
    First we will check the day of the day of the week that the registration
is performed.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{k}{for} \PY{n}{data\PYZus{}set} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{n}{datetime}\PY{o}{.}\PY{n}{date}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{o}{.}\PY{n}{isoweekday}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    ISO weekday means that Monday is "1" and Sunday is "7". Lets see how the
weekdays are distributed.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{n}{weekday\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Monday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tuesday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wednesday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Thursday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Friday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Saturday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sunday}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{n}{week\PYZus{}date\PYZus{}df} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
         \PY{n}{week\PYZus{}date\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{week\PYZus{}date\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{n}{weekday\PYZus{}dict}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{)}
         \PY{n}{week\PYZus{}date\PYZus{}df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}programdata\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row\_indexer,col\_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html\#indexing-view-versus-copy
  

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}75}]:}              target
         weekday            
         Sunday     0.090866
         Friday     0.099494
         Saturday   0.102531
         Thursday   0.118458
         Wednesday  0.121440
         Monday     0.122304
         Tuesday    0.127977
\end{Verbatim}
            
    \begin{quote}
This validates our observation on the fact that the beginning of the
week has higher acceptance while the weekend has lower.
\end{quote}

\textbf{\emph{Business insight:}}

\textbf{People who sign up during the week are more serious about
finalizing their registration. This is specially true in case of people
who register on Tuesday. On the other hand, on Sunday, people might only
be playing around with their apps and have lower chance of progressing
with their registration.}

\textbf{People regisrating on Tuesday and Monday are better options for
sending followup discounts and incentives.}

\textbf{Moreover, we can send coupons and advertisement emails to the
customers in the beginning of the week. This makes Uber more competitive
on the days that customers are more prone to activate their accounts.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <weekday> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \begin{quote}
We also see its statistical significance. We keep this data for future
modelling.
\end{quote}

Let's move to the second extracted feature. Time distance since new year
in weeks. Going back to figure from section 3-3, we saw that in general
there is a meaningful drop in the beginning of the January in terms of
target value.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{success\PYZus{}mean\PYZus{}weekly} \PY{o}{=} \PY{n}{class\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{resample}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{success\PYZus{}mean\PYZus{}weekly}\PY{o}{.}\PY{n}{index}\PY{p}{,}\PY{n}{success\PYZus{}mean\PYZus{}weekly}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dashed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{week average respose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time of registrationt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation} \PY{o}{=} \PY{l+m+mi}{60}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}77}]:} <matplotlib.legend.Legend at 0x1d20e4ba6d8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_145_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see that there is a meaningful drop in the response rate in the first
three days of the observation. The reason could be explained by the
holiday depression and the need that people feel to change their
carreer. Therefore, I am thinking of creating a feature that captures
this part of the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{k}{for} \PY{n}{data\PYZus{}set} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{early\PYZus{}jan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{n}{x}\PY{o}{.}\PY{n}{day}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{early}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{x}\PY{o}{\PYZlt{}}\PY{o}{=}\PY{l+m+mi}{1} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{early\PYZus{}jan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}79}]:} early\_jan
         early    0.058272
         rest     0.114371
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{early\PYZus{}jan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <early\_jan> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \begin{quote}
We see this category is statistically insignificant and we need to drop
it from our data sets.
\end{quote}

    Now let's see how the time difference between the signup date and the
background check date affect the target. In other words, does it mean
that people who performed their background eariler/later tend to have
different behaviors. We will fill the NAs with today's day, which is
\texttt{2018-04-22}. If the background is checked more than 20 days from
the registation we call it late, else we call it early.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{k}{def} \PY{n+nf}{early\PYZus{}late}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{x} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{:}
                 \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{immediate}\PY{l+s+s2}{\PYZdq{}}
             \PY{k}{elif} \PY{l+m+mi}{1} \PY{o}{\PYZlt{}} \PY{n}{x} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{7}\PY{p}{:}
                 \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{early}\PY{l+s+s2}{\PYZdq{}}
             \PY{k}{elif} \PY{l+m+mi}{7} \PY{o}{\PYZlt{}} \PY{n}{x} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{30} \PY{p}{:}
                 \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{marginal}\PY{l+s+s2}{\PYZdq{}}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{late}\PY{l+s+s2}{\PYZdq{}}
         \PY{k}{for} \PY{n}{data\PYZus{}set} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bgc\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bgc\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2018\PYZhy{}04\PYZhy{}22}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bgc\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{timedelta64[D]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}
                 \PY{n+nb}{int}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{early\PYZus{}late}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}82}]:} background\_interval
         late         0.006140
         marginal     0.057032
         early        0.224517
         immediate    0.390535
         Name: target, dtype: float64
\end{Verbatim}
            
    We see that early background checks tend to be much more repsponsive to
the registration.

\textbf{Business Insight: People who permit their background check in
the first day are 4 times as probable as the whole popuation on becoming
an Uber driver. It is imperative to put large amount of incentive and
advertisement to the first day registrator to finish their background
checks. Moreover, if Uber can experdite the background check process, it
is highly recommended to do so.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <background\_interval> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    We perform a very similar analysis on the time that the data is uploaded
by the driver.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{k}{for} \PY{n}{data\PYZus{}set} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2018\PYZhy{}04\PYZhy{}22}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upload\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{timedelta64[D]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}
                 \PY{n+nb}{int}\PY{p}{)}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{early\PYZus{}late}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upload\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}85}]:} upload\_interval
         late         0.006213
         marginal     0.317913
         early        0.728743
         immediate    0.814723
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upload\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <upload\_interval> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \begin{quote}
This is an amazing addition to the feature set. If the driver
immediately upload their information, they have up to 80\% chance of
becoming uber drivers. This is the first feature that captures 80\% of
the driver class.
\end{quote}

\begin{quote}
\textbf{\emph{Business insight:}} \textbf{\emph{In Uber, we need to
assure that the costumers follow up their signup with uploading their
documents. Apparently, not uploading them is the top barrier for
customer following their registration. We might also be able to provide
paralle requirements for documents, so that if the driver doesn't have
one type of identification accessible, he/she can use another one.}}
\end{quote}

Now that I see such a response when it comes to upload interval and
background interval, I was wondering how the interations of these two
variable can help us? If you check your background and upload your data
early, does it mean you will become an Uber driver? Let's check.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}upload\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upload\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}upload\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{missing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}upload\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}87}]:} background\_upload\_interaction
         early\_marginal         0.000000
         immediate\_early        0.000000
         immediate\_marginal     0.000000
         immediate\_immediate    0.093178
         marginal\_early         0.100313
         late\_late              0.106350
         late\_marginal          0.109421
         late\_early             0.109503
         marginal\_marginal      0.112773
         missing                0.113131
         late\_immediate         0.123894
         marginal\_late          0.126984
         early\_early            0.131988
         early\_immediate        0.152083
         marginal\_immediate     0.159483
         early\_late             0.166667
         immediate\_late         0.375000
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}upload\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <background\_upload\_interaction> feature and target is 0.0085.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \begin{quote}
Although such interaction is already included in the tree based
techniques, it is nice to have such fature especially for the linear
models.
\end{quote}

We noticed that in general there is a meaningful pattern in daily
average of target. Therefore, we might be able to use yesterday's target
mean as the mimicker of todays probablity. Let's see if there is any
significance on it. By this we mean, if the target ratio is lower than
average yesterday, can we see it affects todays target ratio?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{n}{daily\PYZus{}target\PYZus{}mean} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Now, we need to extract yesterday mean target from this data. For that,
we simply move the date to tomorrows date. We also forward fill the
first day.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{+}\PY{n}{timedelta}\PY{p}{(}\PY{n}{days}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{columns} \PY{o}{=} \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{n}{january\PYZus{}first} \PY{o}{=} \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2016\PYZhy{}01\PYZhy{}02}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{jan\PYZus{}first\PYZus{}val} \PY{o}{=} \PY{n}{january\PYZus{}first}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{jan\PYZus{}first\PYZus{}day} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2016\PYZhy{}01\PYZhy{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{n}{january\PYZus{}first\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{signup\PYZus{}date}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{jan\PYZus{}first\PYZus{}day}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{jan\PYZus{}first\PYZus{}val}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{30}\PY{p}{]}\PY{p}{)}
         \PY{n}{daily\PYZus{}target\PYZus{}mean} \PY{o}{=} \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{january\PYZus{}first\PYZus{}df}\PY{p}{)}
         \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yesterday\PYZus{}target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}93}]:}   signup\_date  yesterday\_target
         0  2016-01-02          0.058272
         1  2016-01-03          0.097561
         2  2016-01-04          0.103421
         3  2016-01-05          0.131868
         4  2016-01-06          0.134954
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{p}{,} \PY{n}{on} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{daily\PYZus{}target\PYZus{}mean}\PY{p}{,} \PY{n}{on} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yesterday\PYZus{}target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}95}]:} target
         0.0    0.109635
         1.0    0.111738
         Name: yesterday\_target, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yesterday\PYZus{}target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <target> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \begin{quote}
Although there is not much of a differece in target class based on the
mean target value, we still keep the column as it is statistically
significant.
\end{quote}

We previously noticed that none of the drivers who submitted their
background consent or uploaded their data actually become an Uber
driver. These could be interesing featuers we can use. Let's first
develop \texttt{late\_background}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{k}{def} \PY{n+nf}{late\PYZus{}checker}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{x} \PY{o}{==} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2018\PYZhy{}04\PYZhy{}22}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} filling NAs with a time stamp not in our data}
                 \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{missing}\PY{l+s+s2}{\PYZdq{}}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{if} \PY{n}{x} \PY{o}{\PYZgt{}} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2016\PYZhy{}02\PYZhy{}29}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
                     \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{late}\PY{l+s+s2}{\PYZdq{}}
                 \PY{k}{else}\PY{p}{:}
                     \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ontime}\PY{l+s+s2}{\PYZdq{}}
         \PY{k}{for} \PY{n}{data\PYZus{}set} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}background}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bgc\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{late\PYZus{}checker}\PY{p}{)}
             \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}upload}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{late\PYZus{}checker}\PY{p}{)}
             
         \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}97}]:}       id  city\_name  signup\_os  signup\_channel signup\_date   bgc\_date  \textbackslash{}
         0  42795          1          4               1  2016-01-23 2016-02-25   
         1  12767          2          3               1  2016-01-18 2018-04-22   
         2  19141          2          4               1  2016-01-30 2018-04-22   
         3  49638          2          3               1  2016-01-05 2016-01-07   
         4  45812          3          4               2  2016-01-18 2018-04-22   
         
           vehicle\_added\_date  vehicle\_make  vehicle\_model first\_completed\_date  \textbackslash{}
         0         2016-03-01            22             46                  NaT   
         1         2018-04-22             3              1                  NaT   
         2         2018-04-22             3              1                  NaT   
         3         2016-01-12            31             53           2016-01-23   
         4         2018-04-22             3              1                  NaT   
         
               {\ldots}            mpg  duplicated  weekday  early\_jan  background\_interval  \textbackslash{}
         0     {\ldots}      28.318877           2        6       rest                 late   
         1     {\ldots}      28.318877           1        1       rest                 late   
         2     {\ldots}      28.318877           1        6       rest                 late   
         3     {\ldots}      30.860000           2        2       rest                early   
         4     {\ldots}      28.318877           1        1       rest                 late   
         
            upload\_interval  background\_upload\_interaction  yesterday\_target  \textbackslash{}
         0             late                      late\_late          0.111888   
         1             late                  late\_marginal          0.094382   
         2             late                      late\_late          0.110806   
         3            early                  late\_marginal          0.131868   
         4             late                    early\_early          0.094382   
         
            late\_background late\_upload  
         0           ontime        late  
         1          missing     missing  
         2          missing     missing  
         3           ontime      ontime  
         4          missing     missing  
         
         [5 rows x 26 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}background}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}98}]:} late\_background
         missing    0.006473
         late       0.031250
         ontime     0.182905
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}upload}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}99}]:} late\_upload
         missing    0.006330
         late       0.064516
         ontime     0.446516
         Name: target, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}background}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}upload}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <late\_background> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <late\_upload> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \begin{quote}
These two values features seem to be meaningful. We keep them for our
final mode.
\end{quote}

Let's drop the existing time-series variables, since the models cant
understand them. Moreover, let's perform mean encoding for the faetures
that we just created. The features are \texttt{weekday},
\texttt{background\_interval}, \texttt{upload\_interval},
\texttt{background\_upload\_interaction}, \texttt{early\_jan},
\texttt{late\_upload}, \texttt{late\_background}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{k}{for} \PY{n}{data\PYZus{}set} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
              \PY{n}{data\PYZus{}set}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{date\PYZus{}columns}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{n}{time\PYZus{}series\PYZus{}faetures} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}upload}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{late\PYZus{}background}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upload\PYZus{}interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{background\PYZus{}upload\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{early\PYZus{}jan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{n}{df1} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
          \PY{n}{df2} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
          \PY{n}{target} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}
          
          \PY{k}{for} \PY{n}{category} \PY{o+ow}{in} \PY{n}{time\PYZus{}series\PYZus{}faetures}\PY{p}{:}
              \PY{n}{cat\PYZus{}dict} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{df1}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{n}{category}\PY{p}{)}\PY{p}{[}\PY{n}{target}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
              \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{1}
              \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{cat\PYZus{}dict}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                  \PY{n}{cat\PYZus{}dict}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{o}{=} \PY{n}{i}
                  \PY{n}{i} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
              \PY{n}{df1}\PY{p}{[}\PY{n}{category}\PY{p}{]} \PY{o}{=} \PY{n}{df1}\PY{p}{[}\PY{n}{category}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{cat\PYZus{}dict}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{)}
              \PY{n}{df2}\PY{p}{[}\PY{n}{category}\PY{p}{]} \PY{o}{=} \PY{n}{df2}\PY{p}{[}\PY{n}{category}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{if} \PY{n}{x} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{cat\PYZus{}dict}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{n}{cat\PYZus{}dict}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}104}]:} \PY{k}{for} \PY{n}{category} \PY{o+ow}{in} \PY{n}{time\PYZus{}series\PYZus{}faetures}\PY{p}{:}
              \PY{n}{categorical\PYZus{}features\PYZus{}test}\PY{p}{(}\PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{category}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <late\_upload> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <late\_background> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <weekday> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <background\_interval> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <upload\_interval> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <background\_upload\_interaction> feature and target is 0.0085.
This dependance is statistically significant!
--------------------------------------------------------------------------------
*************************** Chi-Square test results!!!***************************
The p-value for the correlation bwteern <early\_jan> feature and target is 0.0.
This dependance is statistically significant!
--------------------------------------------------------------------------------

    \end{Verbatim}

    \subsection{5-4- Model Preprocessing}\label{model-preprocessing}

We now would like to perform standard sacaler. The reason is that the
non-tree base techniques (such as KNN, ANN, linear regression) calculate
some form of distance from variables which emphasizes features with
larger scales. We perform standard scaling. What this does is to
calculate the mean and standard deviation on each feature in training
set, e.g., \(\mu\) for mean, and \(\sigma\) for standard deviation. This
is performed only on training dataset. Then all values in train and test
data set are mapped using the following function to standard normal
distribution: \[ x_{new} = \frac{X_{old}-\mu}{\sigma}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{combined\PYZus{}data}\PY{p}{:}
              \PY{n}{dataset}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{target}
          
          \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{combined\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{target}
          
          \PY{n}{columns} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
          \PY{n}{index\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{values}
          \PY{n}{index\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}107}]:} \PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
          \PY{n}{scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
          \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}train}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{columns}\PY{p}{)}
          \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}test}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{columns}\PY{p}{)}
\end{Verbatim}


    I like to keep the train and test data as a seperate csv file for later
use.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}108}]:} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \subsection{5-5- Feature Importance and Feature
Selection}\label{feature-importance-and-feature-selection}

We have generated several features. We need to quickly see if they are
all important, and which one is more important. If some features don't
carry any information we can drop them to have less complex model that
is prone less to overfitting.

Let's first see how much of variance explained with features and if we
can keep less of them. Before that, we will standard scale the data to
have smoother looking PCA.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{p}{)}
          \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{} fitting PCA on train data}
          \PY{n}{var} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}} \PY{c+c1}{\PYZsh{} amount of variance that each feature explains}
          \PY{n}{cummulative\PYZus{}explained\PYZus{}variance} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{,} \PY{n}{decimals}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cummulative\PYZus{}explained\PYZus{}variance}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{brown}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dashed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                   \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Principal Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Explained Variance (}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}109}]:} Text(0,0.5,'Explained Variance (\%)')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_190_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
Looking at this figure, we see that all, except the last three features,
of our features are required to explain the whole variance in the
feature set. This is a very good news, as features tend to capture
different part of the variance and therefore, we can rely on their
non-redunduncy. Therefore, there is no need for feature removal. We
assure enough regularization for each model so that none of the models
are impacted by the less informative features.
\end{quote}

Let's fit a random forest and quickly check which one of our features
are more informative. This will direct our attention to the part of the
feature set that carries discriminative information.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{n}{forest} \PY{o}{=} \PY{n}{ExtraTreesClassifier}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{2000}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
          \PY{n}{features} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
          \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}
          \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}train}
          \PY{n}{forest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
          \PY{n}{importances} \PY{o}{=} \PY{n}{forest}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}
          \PY{n}{std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{[}\PY{n}{tree}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}} \PY{k}{for} \PY{n}{tree} \PY{o+ow}{in} \PY{n}{forest}\PY{o}{.}\PY{n}{estimators\PYZus{}}\PY{p}{]}\PY{p}{,}
                       \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
          \PY{n}{indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{importances}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Print the feature ranking}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature ranking:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          \PY{k}{for} \PY{n}{f} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{. feature }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{ (}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{f} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{indices}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{p}{,} \PY{n}{importances}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{p}{]}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Plot the feature importances of the forest}
          \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature importances}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{importances}\PY{p}{[}\PY{n}{indices}\PY{p}{]}\PY{p}{,}
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{indices}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Feature ranking:
1. feature 15 (0.341729)
upload\_interval

2. feature 19 (0.163373)
late\_upload

3. feature 6 (0.146313)
vehicle\_added\_date\_available

4. feature 14 (0.093182)
background\_interval

5. feature 7 (0.060719)
vehicle\_age

6. feature 4 (0.053970)
vehicle\_model

7. feature 3 (0.052433)
vehicle\_make

8. feature 9 (0.049060)
origin

9. feature 8 (0.010352)
web\_paid\_interaction

10. feature 11 (0.008660)
duplicated

11. feature 2 (0.006892)
signup\_channel

12. feature 18 (0.003399)
late\_background

13. feature 5 (0.003014)
bcg\_checked

14. feature 1 (0.002935)
signup\_os

15. feature 10 (0.002336)
mpg

16. feature 0 (0.000526)
city\_name

17. feature 12 (0.000488)
weekday

18. feature 17 (0.000409)
yesterday\_target

19. feature 16 (0.000201)
background\_upload\_interaction

20. feature 13 (0.000008)
early\_jan


    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_192_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Looking at the PCA and RF importance results, we notice that the first
11 features carry most of the information required to predict the
target. However, there is still some variance being explained by the
rest. We assure proper regularization for each model so that we don't
overfit our model.

    \section{6- Modelling}\label{modelling}

Now let's get to the exciting part, modelling the technique and see how
can we hypertune it for better result and get the final prediction.

\subsection{6-1- Simple Model to Check
Bias/Variance}\label{simple-model-to-check-biasvariance}

We first fit a simple logistic regression to see how our model is doing
on train and test data and see if, before going to the complication of
modelling, we can take care of some issues to improve the model.

We use logistic regression here due to its simplicity and running time
inexpensiveness. Not specifying any regularization, also, allows to
assure that possilbe overfitting is caught. Let's build the model and
see the reslts.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}111}]:} \PY{n}{logreg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
          \PY{n}{logreg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{test\PYZus{}acc\PYZus{}log} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{logreg}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
          \PY{n}{train\PYZus{}acc\PYZus{}log} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{logreg}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The test accuracy is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{test\PYZus{}acc\PYZus{}log}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The training accuracy for simple logistic regression is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{train\PYZus{}acc\PYZus{}log}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The test accuracy is 93.8
The training accuracy for simple logistic regression is 93.65

    \end{Verbatim}

    Accuracy on train and test data is pretty close. This means there is not
much of over fitting in our model. Therefore, we are sure that what our
model missess from becoming more accurate is more features.
\textbf{Business insight: more seperating feature from the customer
could help this analysis largely, some examples are gender, time of the
day of the signup, returning IP, age, etc.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}Computing false and true positive rates}
          
          \PY{n}{probs} \PY{o}{=} \PY{n}{logreg}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          \PY{n}{preds} \PY{o}{=} \PY{n}{probs}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
          \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{threshold} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
          \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Receiver Operating Characteristic \PYZhy{} Logistic Regression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC = }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{roc\PYZus{}auc}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,}\PY{n}{fpr}\PY{p}{,} \PY{n}{facecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{violet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_197_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{quote}
We reached \(AUC=0.97\) with this simple model which is acceptable. Also
since there is not much of difference between train and test accuracy,
we can say that the model \textbf{generalizes} well. It is suggested
that some basic features like gender, age, and marriage status would
become insightful in seperating different groups and reach a better AUC.
\end{quote}

    \subsection{6-2- SVM, KNN, and Random Forest Hyperparameter
Tunning}\label{svm-knn-and-random-forest-hyperparameter-tunning}

Now that we deomnstrate the feature set strength, we need to check if
more complex models can handle this data better. What we do here is,
before jumping to model, we will perform cross validation and gridsearch
on the training set to achieve best parameters to be used in the models.

Support vector machine, k-nearest neighbors, and random forest are used
here. The hypertuning cells are commented as they take long time to run.
You can uncomment them if you want to get the results. I provide the
results in the next cell.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{c+c1}{\PYZsh{}parameters=\PYZob{}\PYZsq{}kernel\PYZsq{}:[\PYZsq{}linear\PYZsq{},\PYZsq{}rbf\PYZsq{}],\PYZsq{}C\PYZsq{}:[0.1,.3,1,3,10,30]\PYZcb{}}
          \PY{c+c1}{\PYZsh{}scoring = \PYZob{}\PYZsq{}AUC\PYZsq{}: \PYZsq{}roc\PYZus{}auc\PYZsq{}, \PYZsq{}Accuracy\PYZsq{}: make\PYZus{}scorer(accuracy\PYZus{}score)\PYZcb{}}
          \PY{c+c1}{\PYZsh{}svc=SVC()}
          \PY{c+c1}{\PYZsh{}clf\PYZus{}svc=GridSearchCV(estimator=svc,param\PYZus{}grid=parameters, scoring=scoring, cv=3, refit=\PYZsq{}AUC\PYZsq{})}
          \PY{c+c1}{\PYZsh{}clf\PYZus{}svc.fit(X\PYZus{}train, y\PYZus{}train)}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best parameters are \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}svc.best\PYZus{}params\PYZus{}))}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best score is \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}svc.best\PYZus{}score\PYZus{}))}
\end{Verbatim}


    This is the results we get from hypertuning the SVM:

\textbf{\emph{Best parameters
are:}}\texttt{\{\textquotesingle{}C\textquotesingle{}:\ 30,\ \textquotesingle{}kernel\textquotesingle{}:\ \textquotesingle{}linear\textquotesingle{}\}}

\textbf{Best score is 0.964985864043404}

This also gives us some more insights: \textbf{since the kernel chosen
is linear, we can see that the features help us pretty much in
seperating different classes. Moreover, we can see that the smaller
regularization (1/C) is chosen. This means that our model is doing very
well in not overfitting. The result is also consistent with the results
we got from PCA analysis of features.}

In case you are interested in getting the results yourself, uncomment
the cell above.

    Let's check the KNN and cross validation on it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{c+c1}{\PYZsh{}t\PYZus{}start = time.time()}
          
          \PY{c+c1}{\PYZsh{}scoring = \PYZob{}\PYZsq{}AUC\PYZsq{}: \PYZsq{}roc\PYZus{}auc\PYZsq{}, \PYZsq{}Accuracy\PYZsq{}: make\PYZus{}scorer(accuracy\PYZus{}score)\PYZcb{}}
          \PY{c+c1}{\PYZsh{}knn=KNeighborsClassifier(n\PYZus{}jobs=\PYZhy{}1)}
          \PY{c+c1}{\PYZsh{}parameters=\PYZob{}\PYZsq{}n\PYZus{}neighbors\PYZsq{}:[10,100],\PYZsq{}weights\PYZsq{}:[\PYZsq{}uniform\PYZsq{}, \PYZsq{}distance\PYZsq{}]\PYZcb{}}
          \PY{c+c1}{\PYZsh{}clf\PYZus{}knn=GridSearchCV(estimator=knn,param\PYZus{}grid=parameters, scoring=scoring, cv=3, refit=\PYZsq{}AUC\PYZsq{})}
          \PY{c+c1}{\PYZsh{}clf\PYZus{}knn.fit(X\PYZus{}train, y\PYZus{}train)}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best parameters are \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}knn.best\PYZus{}params\PYZus{}))}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best score is \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}knn.best\PYZus{}score\PYZus{}))}
          
          \PY{c+c1}{\PYZsh{}t\PYZus{}end = time.time()}
          
          
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}It took \PYZob{}\PYZcb{} seconds to run this test\PYZdq{}.format(t\PYZus{}end\PYZhy{}t\PYZus{}start))}
\end{Verbatim}


    This is the results we get from for the KNN:

Best parameters are
\texttt{\{\textquotesingle{}n\_neighbors\textquotesingle{}:\ 100,\ \textquotesingle{}weights\textquotesingle{}:\ \textquotesingle{}uniform\textquotesingle{}\}}
Best score is \texttt{0.9643999007173726} It took
\texttt{437.0903055667877} seconds to run this test.

If you are interested in getting the results, uncomment the cell block
above. We will use these parameters to check the generizability of our
knn-model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{c+c1}{\PYZsh{}t\PYZus{}start = time.time()}
          
          \PY{c+c1}{\PYZsh{}scoring = \PYZob{}\PYZsq{}AUC\PYZsq{}: \PYZsq{}roc\PYZus{}auc\PYZsq{}, \PYZsq{}Accuracy\PYZsq{}: make\PYZus{}scorer(accuracy\PYZus{}score)\PYZcb{}}
          \PY{c+c1}{\PYZsh{}rf=RandomForestClassifier(n\PYZus{}jobs=\PYZhy{}1)}
          \PY{c+c1}{\PYZsh{}param\PYZus{}grid = \PYZob{}\PYZdq{}max\PYZus{}depth\PYZdq{}: [7, None],}
                        \PY{c+c1}{\PYZsh{}\PYZdq{}max\PYZus{}features\PYZdq{}: [\PYZdq{}sqrt\PYZdq{}, \PYZdq{}log2\PYZdq{}],}
                        \PY{c+c1}{\PYZsh{}\PYZdq{}min\PYZus{}samples\PYZus{}split\PYZdq{}: [100,200],}
                        \PY{c+c1}{\PYZsh{}\PYZdq{}min\PYZus{}samples\PYZus{}leaf\PYZdq{}: [10, 50],}
                        \PY{c+c1}{\PYZsh{}\PYZdq{}bootstrap\PYZdq{}: [True, False],}
                        \PY{c+c1}{\PYZsh{}\PYZdq{}criterion\PYZdq{}: [\PYZdq{}gini\PYZdq{}, \PYZdq{}entropy\PYZdq{}]\PYZcb{}}
          \PY{c+c1}{\PYZsh{}clf\PYZus{}rf = GridSearchCV(rf, param\PYZus{}grid=param\PYZus{}grid,scoring=scoring, cv=3, refit=\PYZsq{}AUC\PYZsq{})}
          \PY{c+c1}{\PYZsh{}clf\PYZus{}rf.fit(X\PYZus{}train, y\PYZus{}train)}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best parameters are \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}rf.best\PYZus{}params\PYZus{}))}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best score is \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}rf.best\PYZus{}score\PYZus{}))}
          \PY{c+c1}{\PYZsh{}t\PYZus{}end = time.time()}
          \PY{c+c1}{\PYZsh{}print(\PYZdq{}It took \PYZob{}\PYZcb{} seconds to run this test\PYZdq{}.format(t\PYZus{}end\PYZhy{}t\PYZus{}start))}
\end{Verbatim}


    This is the results we get for the random forest:

Best parameters are
\texttt{\{\textquotesingle{}bootstrap\textquotesingle{}:\ True,\ \textquotesingle{}criterion\textquotesingle{}:\ \textquotesingle{}entropy\textquotesingle{},\ \textquotesingle{}max\_depth\textquotesingle{}:\ 7,\ \textquotesingle{}max\_features\textquotesingle{}:\ 10,\ \textquotesingle{}min\_samples\_leaf\textquotesingle{}:\ 10,\ \textquotesingle{}min\_samples\_split\textquotesingle{}:\ 100\}}

Best score is 0.9698143454900715 It took 551.6602492332458 seconds to
run this test.

If you are interested in getting the results, uncomment the cell block
above. We will use these parameters to check the generizability of our
random forest model.

    \subsection{6-3- SVM, KNN, and Random Forest
Results}\label{svm-knn-and-random-forest-results}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{svc2} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{probability}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{svc2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{probs} \PY{o}{=} \PY{n}{svc2}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
        \PY{n}{preds} \PY{o}{=} \PY{n}{probs}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{threshold} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
        \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Receiver Operating Characteristic \PYZhy{} Support Vector Machine}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC = }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{roc\PYZus{}auc}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,}\PY{n}{fpr}\PY{p}{,} \PY{n}{facecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{violet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{knn2}\PY{o}{=}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{weights}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uniform}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{knn2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{probs} \PY{o}{=} \PY{n}{knn2}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
        \PY{n}{preds} \PY{o}{=} \PY{n}{probs}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{threshold} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
        \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Receiver Operating Characteristic \PYZhy{} K\PYZhy{}Nearest Neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC = }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{roc\PYZus{}auc}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,}\PY{n}{fpr}\PY{p}{,} \PY{n}{facecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{violet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{rf2}\PY{o}{=}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{bootstrap}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{criterion}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=} \PY{l+m+mi}{7}\PY{p}{,}
                                   \PY{n}{max\PYZus{}features}\PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}
                                   \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{rf2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{probs} \PY{o}{=} \PY{n}{rf2}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
        \PY{n}{preds} \PY{o}{=} \PY{n}{probs}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{threshold} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
        \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Receiver Operating Characteristic \PYZhy{} Random Forest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC = }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{roc\PYZus{}auc}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,}\PY{n}{fpr}\PY{p}{,} \PY{n}{facecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{violet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \subsection{6-4- Gradient Boosted Trees as a Boosting
Technique}\label{gradient-boosted-trees-as-a-boosting-technique}

Just before we finish, and although our data showed marginal improvement
when more complicate models are used, let's give the gradient boosted
trees a chance to see how boosting trees would affect the results. Here
is how we perform hypertuning using gridsearchcv.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}t\PYZus{}start = time.time()}
        \PY{c+c1}{\PYZsh{}X\PYZus{}train = pd.read\PYZus{}csv(\PYZdq{}X\PYZus{}train.csv\PYZdq{}).drop(columns=[\PYZdq{}Unnamed: 0\PYZdq{},\PYZdq{}index\PYZdq{}], axis=1)}
        \PY{c+c1}{\PYZsh{}y\PYZus{}train = pd.read\PYZus{}csv(\PYZdq{}y\PYZus{}train.csv\PYZdq{}).drop(columns=[\PYZdq{}Unnamed: 0\PYZdq{},\PYZdq{}id\PYZdq{}], axis=1).target.values}
        \PY{c+c1}{\PYZsh{}parameters = \PYZob{}\PYZsq{}min\PYZus{}child\PYZus{}weight\PYZsq{}: [5, 10],}
        \PY{c+c1}{\PYZsh{}        \PYZsq{}gamma\PYZsq{}: [0.01, .1,1],}
        \PY{c+c1}{\PYZsh{}        \PYZsq{}subsample\PYZsq{}: [0.8,1],}
        \PY{c+c1}{\PYZsh{}        \PYZsq{}colsample\PYZus{}bytree\PYZsq{}: [0.6, 0.8],}
        \PY{c+c1}{\PYZsh{}        \PYZsq{}max\PYZus{}depth\PYZsq{}: [3, 5,  7]}
        \PY{c+c1}{\PYZsh{}        \PYZcb{}}
        \PY{c+c1}{\PYZsh{}xgbc = XGBClassifier(objective=\PYZsq{}binary:logistic\PYZsq{},}
        \PY{c+c1}{\PYZsh{}                    silent=True, verbose=True, scale\PYZus{}pos\PYZus{}weight=8)\PYZsh{} \PYZsq{}scale\PYZus{}pos\PYZus{}weight\PYZsq{} to set as the ratio for skewed data}
        
        \PY{c+c1}{\PYZsh{}clf\PYZus{}xgbc=GridSearchCV(estimator=xgbc,param\PYZus{}grid=parameters, scoring=\PYZsq{}roc\PYZus{}auc\PYZsq{}, cv=3, n\PYZus{}jobs=1,verbose=100)}
        \PY{c+c1}{\PYZsh{}clf\PYZus{}xgbc.fit(X\PYZus{}train, y\PYZus{}train)}
        
        \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best parameters are \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}xgbc.best\PYZus{}params\PYZus{}))}
        \PY{c+c1}{\PYZsh{}print(\PYZdq{}Best score is \PYZob{}\PYZcb{}\PYZdq{}.format(clf\PYZus{}xgbc.best\PYZus{}score\PYZus{}))}
        
        \PY{c+c1}{\PYZsh{}t\PYZus{}end = time.time()}
        \PY{c+c1}{\PYZsh{}print(\PYZdq{}It took \PYZob{}\PYZcb{} seconds to run this test\PYZdq{}.format(t\PYZus{}end\PYZhy{}t\PYZus{}start))}
\end{Verbatim}


    And here is the results for XGBoost classifier hyper tuning.

\textbf{Best parameters are}
\texttt{\{\textquotesingle{}colsample\_bytree\textquotesingle{}:\ 0.6,\ \textquotesingle{}gamma\textquotesingle{}:\ 30,\ \textquotesingle{}learning\_rate\textquotesingle{}:\ 0.02,\ \textquotesingle{}max\_depth\textquotesingle{}:\ 6,\ \textquotesingle{}min\_child\_weight\textquotesingle{}:\ 5,\ \textquotesingle{}n\_estimators\textquotesingle{}:\ 1000,\ \textquotesingle{}subsample\textquotesingle{}:\ 0.6\}}

\textbf{Best score is} \texttt{0.9702629524162981}

It took \texttt{926.2014994621277\ seconds} to run this test.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{objective}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{binary:logistic}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eval\PYZus{}metric}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{auc}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gamma}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{30}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eta}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{0.02}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{min\PYZus{}child\PYZus{}weight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{subsample}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{0.6}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{colsample\PYZus{}bytree}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{0.8}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{scale\PYZus{}pos\PYZus{}weight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{1.0}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{silent}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{max\PYZus{}depth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{6}
        \PY{n}{params}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}jobs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
        
        \PY{n}{plst} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{params}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{xgtrain} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{DMatrix}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{)}
        \PY{n}{xgval} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{DMatrix}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}train using early stopping and predict}
        
        \PY{n}{watchlist} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{xgtrain}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{xgval}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
        
        \PY{n}{num\PYZus{}rounds} \PY{o}{=} \PY{l+m+mi}{1000}
        
        
        \PY{n}{model} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{plst}\PY{p}{,} \PY{n}{xgtrain}\PY{p}{,} \PY{n}{num\PYZus{}rounds}\PY{p}{,}\PY{n}{watchlist}\PY{p}{,} \PY{n}{early\PYZus{}stopping\PYZus{}rounds}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{verbose\PYZus{}eval}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        
        \PY{n}{preds1} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{xgval}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{loop} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
            \PY{n}{model} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{plst}\PY{p}{,} \PY{n}{xgtrain}\PY{p}{,} \PY{n}{num\PYZus{}rounds}\PY{p}{,}\PY{n}{watchlist}\PY{p}{,} \PY{n}{early\PYZus{}stopping\PYZus{}rounds}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{verbose\PYZus{}eval}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
            \PY{n}{preds1} \PY{o}{=} \PY{n}{preds1} \PY{o}{+} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{xgval}\PY{p}{)}
        \PY{n}{preds} \PY{o}{=} \PY{p}{(}\PY{n}{preds1}\PY{o}{/}\PY{l+m+mi}{101} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{threshold} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
        \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Receiver Operating Characteristic \PYZhy{} K\PYZhy{}Nearest Neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC = }\PY{l+s+si}{\PYZpc{}0.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{roc\PYZus{}auc}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,}\PY{n}{fpr}\PY{p}{,} \PY{n}{facecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{violet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Even a more complex classifier like XGBoost doesn't improve our
technique significantly. Therefore, we conclude that what we need here
is more data.

    \section{Details - Task 3:}\label{details---task-3}

Build a model to forecast the number of new drivers we expect to start
every week. How would you validate a model like this? What other
information would you use if you had access to all of Uber's data?

    For this question, we need to generate a new dataset from what we have
from the beginning. The main data is a time series that has the number
of drivers as the value to predict. Since we are adding data as we go **
we will combine the EDA and Feature Engineering steps**.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{regr\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ds\PYZus{}challenge\PYZus{}v2\PYZus{}data.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{usecols}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PYZbs{}
        \PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
        \PY{n}{regr\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{regr\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{infer\PYZus{}datetime\PYZus{}format}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{num\PYZus{}drivers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{regr\PYZus{}data} \PY{o}{=} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    We, now, have daily counts of the drivers who become Uber drivers. We,
however, need the weekly counts.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{regr\PYZus{}data} \PY{o}{=} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{resample}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{W}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{regr\PYZus{}data}
\end{Verbatim}


    We are down to 9 observation. Let's see how this weekly aggregate
changes over time.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dashed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekly acitivation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{)}
\end{Verbatim}


    We see an obvious pattern:

\begin{quote}
As we start the registration people start to become driver more and
more, and this section of data is quite linear. And as we finish the
registration (end of January), it will start dropping. So an intuitive
option is to see how many registration we have in the last thirty days.
\end{quote}

I will import a new dataset that has all the data so that we can use it
for further.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ds\PYZus{}challenge\PYZus{}v2\PYZus{}data.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{date\PYZus{}column} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bgc\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vehicle\PYZus{}added\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first\PYZus{}completed\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{k}{for} \PY{n}{date\PYZus{}column} \PY{o+ow}{in} \PY{n}{date\PYZus{}column}\PY{p}{:}
            \PY{n}{data}\PY{p}{[}\PY{n}{date\PYZus{}column}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{date\PYZus{}column}\PY{p}{]}\PY{p}{,}\PY{n}{infer\PYZus{}datetime\PYZus{}format}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    First, I would like to see what is the cumulative previous 30 days of
number of registration. I also need to add the rest of the date indexes
that are available on the activation date and not on the registration
date.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{daily\PYZus{}signup} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{id}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
        \PY{n}{daily\PYZus{}signup}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}num}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
        \PY{n}{daily\PYZus{}signup}\PY{o}{.}\PY{n}{tail}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{quote}
As we see at the tail of this column, the registration data doesn't
extend to th end of the activation data which is March 3rd. Therefore,
we need to extend the dataframe to that date.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} adding data range that is not available}
        \PY{n}{idx} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{date\PYZus{}range}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2016\PYZhy{}01\PYZhy{}31}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2016\PYZhy{}03\PYZhy{}06}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{added\PYZus{}date} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}num}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{idx}\PY{p}{)}
        \PY{n}{added\PYZus{}date} \PY{o}{=} \PY{n}{added\PYZus{}date}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
        \PY{n}{added\PYZus{}date}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{date}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{signup\PYZus{}num}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{added\PYZus{}date}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}num}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{added\PYZus{}date}\PY{o}{.}\PY{n}{tail}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{daily\PYZus{}signup} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{daily\PYZus{}signup}\PY{p}{,}\PY{n}{added\PYZus{}date}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{daily\PYZus{}signup}\PY{o}{.}\PY{n}{tail}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Now we calculate the past 30 days number of registrations and then plot
it over time.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{daily\PYZus{}signup}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}num}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{daily\PYZus{}signup}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{rolling}\PY{p}{(}\PY{n}{window}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,}\PY{n}{min\PYZus{}periods}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{signup\PYZus{}num}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{daily\PYZus{}signup}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{markerfacecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{1} \PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{past month rolling sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of registrations in past 30 days}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{regr\PYZus{}data} \PY{o}{=} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{daily\PYZus{}signup}\PY{p}{,} \PY{n}{on} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{date}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{left}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    We noticed that the trend is very similar to the trend of number of
activations. We might use the data from this feature to predict the
number of drivers. Let's see how these two columns are in a scatter
plot.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{num\PYZus{}drivers}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Past 30 day \PYZsh{} of registration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of drivers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    We see a beautifull linear relation between the past 30 days number of
registration and number of drivers. Since we only have 9 observations,
we use bootstrapping to assure that the outliers don't dominate our
study.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{draw\PYZus{}bs\PYZus{}pairs\PYZus{}linreg}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Perform pairs bootstrap for linear regression.\PYZdq{}\PYZdq{}\PYZdq{}}
        
            \PY{c+c1}{\PYZsh{} Set up array of indices to sample from: inds}
            \PY{n}{inds} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Initialize replicates: bs\PYZus{}slope\PYZus{}reps, bs\PYZus{}intercept\PYZus{}reps}
            \PY{n}{bs\PYZus{}slope\PYZus{}reps} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{size}\PY{p}{)}
            \PY{n}{bs\PYZus{}intercept\PYZus{}reps} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{size}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Generate replicates}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{size}\PY{p}{)}\PY{p}{:}
                \PY{n}{bs\PYZus{}inds} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{inds}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{inds}\PY{p}{)}\PY{p}{)}
                \PY{n}{bs\PYZus{}x}\PY{p}{,} \PY{n}{bs\PYZus{}y} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{n}{bs\PYZus{}inds}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{n}{bs\PYZus{}inds}\PY{p}{]}
                \PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{bs\PYZus{}intercept\PYZus{}reps}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polyfit}\PY{p}{(}\PY{n}{bs\PYZus{}x}\PY{p}{,} \PY{n}{bs\PYZus{}y}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
        
            \PY{k}{return} \PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{,} \PY{n}{bs\PYZus{}intercept\PYZus{}reps}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{,} \PY{n}{bs\PYZus{}intercept\PYZus{}reps} \PY{o}{=} \PY{n}{draw\PYZus{}bs\PYZus{}pairs\PYZus{}linreg}\PY{p}{(}\PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{num\PYZus{}drivers}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Compute and print 95\PYZpc{} CI for slope}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{2.5}\PY{p}{,}\PY{l+m+mf}{97.5}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    We see that the confidence interval for the slope of the regression is
quite acceptable and within a small range. However, in ordet to assure
that, we first need to see the graph of the bootstrap slopes and also
take a look at the \(R^2\) of the relationship between parameters.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Plot the bootstrap lines}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
            \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{bs\PYZus{}intercept\PYZus{}reps}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}
                         \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{num\PYZus{}drivers}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Past 30 day \PYZsh{} of registration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of drivers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{polyfit}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{degree}\PY{p}{)}\PY{p}{:}
            \PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        
            \PY{n}{coeffs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polyfit}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{degree}\PY{p}{)}
        
             \PY{c+c1}{\PYZsh{} Polynomial Coefficients}
            \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{polynomial}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{coeffs}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} r\PYZhy{}squared}
            \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{poly1d}\PY{p}{(}\PY{n}{coeffs}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} fit values, and mean}
            \PY{n}{yhat} \PY{o}{=} \PY{n}{p}\PY{p}{(}\PY{n}{x}\PY{p}{)}                         \PY{c+c1}{\PYZsh{} or [p(z) for z in x]}
            \PY{n}{ybar} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}          \PY{c+c1}{\PYZsh{} or sum(y)/len(y)}
            \PY{n}{ssreg} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{yhat}\PY{o}{\PYZhy{}}\PY{n}{ybar}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}   \PY{c+c1}{\PYZsh{} or sum([ (yihat \PYZhy{} ybar)**2 for yihat in yhat])}
            \PY{n}{sstot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{ybar}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}    \PY{c+c1}{\PYZsh{} or sum([ (yi \PYZhy{} ybar)**2 for yi in y])}
            \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{ssreg} \PY{o}{/} \PY{n}{sstot}
        
            \PY{k}{return} \PY{n}{results}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{polyfit}\PY{p}{(}\PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,}\PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{num\PYZus{}drivers}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    I think 98\% variance being explained is enough for a model with 9
observations. This was just a feasibility study. Now, since we also want
to validate our results, I will drop one row randomly and keep it for
prediction.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{dropped\PYZus{}row\PYZus{}index} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} 
        \PY{n}{dropped\PYZus{}row} \PY{o}{=} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{dropped\PYZus{}row\PYZus{}index}\PY{p}{]}
        \PY{n}{remaining\PYZus{}rows} \PY{o}{=} \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{dropped\PYZus{}row\PYZus{}index}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{,} \PY{n}{bs\PYZus{}intercept\PYZus{}reps} \PY{o}{=} \PY{n}{draw\PYZus{}bs\PYZus{}pairs\PYZus{}linreg}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{remaining\PYZus{}rows}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{)}\PY{p}{,}
                                                                \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{remaining\PYZus{}rows}\PY{o}{.}\PY{n}{num\PYZus{}drivers}\PY{p}{)}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{1300}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Compute and print 95\PYZpc{} CI for slope}
        \PY{n}{min\PYZus{}slope}\PY{p}{,} \PY{n}{median\PYZus{}slope}\PY{p}{,} \PY{n}{max\PYZus{}slope} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{bs\PYZus{}slope\PYZus{}reps}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{2.5}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mf}{97.5}\PY{p}{]}\PY{p}{)}
        \PY{n}{min\PYZus{}intercept}\PY{p}{,} \PY{n}{median\PYZus{}intercept}\PY{p}{,} \PY{n}{max\PYZus{}intercept} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{bs\PYZus{}intercept\PYZus{}reps}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{2.5}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mf}{97.5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{fig1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{median\PYZus{}intercept} \PY{o}{+} \PY{n}{median\PYZus{}slope} \PY{o}{*} \PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}
        \PY{n}{y\PYZus{}actual} \PY{o}{=} \PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{num\PYZus{}drivers}
        
        \PY{n}{y\PYZus{}upper} \PY{o}{=} \PY{n}{max\PYZus{}intercept}  \PY{o}{+} \PY{n}{max\PYZus{}slope} \PY{o}{*} \PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}
        \PY{n}{y\PYZus{}lower} \PY{o}{=} \PY{n}{min\PYZus{}intercept} \PY{o}{+} \PY{n}{min\PYZus{}slope} \PY{o}{*} \PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,}\PY{n}{y\PYZus{}predict} \PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{predicted test data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s} \PY{o}{=}\PY{l+m+mi}{120}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,}\PY{n}{y\PYZus{}actual} \PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{magenta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{actual data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s} \PY{o}{=} \PY{l+m+mi}{120}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{remaining\PYZus{}rows}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,} \PY{n}{remaining\PYZus{}rows}\PY{o}{.}\PY{n}{num\PYZus{}drivers}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{s} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{)}
        
        
        \PY{c+c1}{\PYZsh{} to include uncertaintly due to small number of observations}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,}\PY{n}{y\PYZus{}upper} \PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                    \PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{95}\PY{l+s+si}{\PYZpc{} c}\PY{l+s+s2}{onfidence interval upper bound}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s} \PY{o}{=}\PY{l+m+mi}{80}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Upper bound}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{(}\PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{o}{+}\PY{l+m+mi}{20}\PY{p}{,}\PY{n}{y\PYZus{}upper}\PY{o}{+}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)}
        
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{p}{,}\PY{n}{y\PYZus{}lower} \PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                    \PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{95}\PY{l+s+si}{\PYZpc{} c}\PY{l+s+s2}{onfidence interval lower bound}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s} \PY{o}{=}\PY{l+m+mi}{80}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lower bound}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{(}\PY{n}{dropped\PYZus{}row}\PY{o}{.}\PY{n}{signup\PYZus{}num}\PY{o}{+}\PY{l+m+mi}{20}\PY{p}{,}\PY{n}{y\PYZus{}lower}\PY{o}{+}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)}
        
        
        
        \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{)}
        
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{median\PYZus{}slope} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{median\PYZus{}intercept}\PY{p}{,}
                         \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mf}{2.5}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Past 30 day \PYZsh{} of registration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of drivers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{For the week ending at }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ with actual number of drivers = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, the model predicted }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ drivers with }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+si}{\PYZpc{} e}\PY{l+s+s2}{rror.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
        \PY{n}{regr\PYZus{}data}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{dropped\PYZus{}row\PYZus{}index}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}actual}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{p}{(}\PY{n}{y\PYZus{}actual}\PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}predict}\PY{p}{)}\PY{o}{/}\PY{n}{y\PYZus{}actual}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \textbf{Caveat: With bigger data set we might see more patterns that
require more features to explain the variance. Therefore, for this task
larget number of observations is essential.}

    I'd like to also add that I went ahead and perform an ARIMA model on the
daily variation of number of UBER drivers. However, the results didn't
look acceptable.

\textbf{Caveat: Since there is no periodicity seen in the data (which
means we havent yet captured enough information), time series prediction
models such as ARIMA tend to be inefficient. Therefore, for this
particular example, we needed to use historical information captured
from the main data set to mimic the results.}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
